<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>arapaima.uk </title>
    <link>http://arapaima.uk/</link>
    <language>en-us</language>
    <author>Gavin Campbell</author>
    <rights>(C) 2018</rights>
    <updated>2018-02-27 00:00:00 &#43;0000 UTC</updated>

    
      
    
      
    
      
        <item>
          <title>Hosting your reveal.js presentations in a subfolder of your Github pages site</title>
          <link>http://arapaima.uk/post/2018-02-27-hosting-multiple-reveal-js-presentations-github-pages/</link>
          <pubDate>Tue, 27 Feb 2018 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2018-02-27-hosting-multiple-reveal-js-presentations-github-pages/</guid>
          <description>

&lt;p&gt;Someone recently asked me how I did this so I thought I&amp;rsquo;d note it down here in case it&amp;rsquo;s of use to anyone else.&lt;/p&gt;

&lt;p&gt;There are a couple of things which I think are prerequisites, namely that this site &lt;a href=&#34;http://arapaima.uk&#34;&gt;arapaima.uk&lt;/a&gt; is &lt;a href=&#34;http://arapaima.uk/fixed/blogsetup/#github-setup&#34;&gt;hosted on GitHub Pages&lt;/a&gt;, and has a &lt;a href=&#34;https://help.github.com/articles/using-a-custom-domain-with-github-pages/&#34;&gt;custom domain&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most of the &lt;a href=&#34;https://revealjs.com&#34;&gt;reveal.js&lt;/a&gt; slides for the public talks I&amp;rsquo;ve done, at least recently, are themselves hosted on Github, in individual repos such as &lt;a href=&#34;https://github.com/arapaima-uk/slides-tsqlt-groupby&#34;&gt;this one&lt;/a&gt; or &lt;a href=&#34;https://github.com/arapaima-uk/database-cd-ssdt-vsts&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;tags&#34;&gt;Tags&lt;/h2&gt;

&lt;p&gt;Now for the science bit. After I&amp;rsquo;ve finished with a talk, I create a &lt;a href=&#34;https://git-scm.com/book/en/v2/Git-Basics-Tagging&#34;&gt;tag&lt;/a&gt; in the repo to indicate which event it was from. Tags are created with the syntax&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git tag -a SQLBits2018 -m &lt;span class=&#34;s2&#34;&gt;&amp;#34;SQLBits 2018, February 24 2018&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and can be reviewed by typing &lt;code&gt;git tag&lt;/code&gt; to see a list of tags, or &lt;code&gt;git show SQLBits2018&lt;/code&gt; to see the commit to which the tag refers:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD database-cd-ssdt-vsts&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ git show SQLBits2018 
tag SQLBits2018
Tagger: Gavin Campbell &amp;lt;gavin@arapaima.uk&amp;gt;
Date:   Tue Feb &lt;span class=&#34;m&#34;&gt;27&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;17&lt;/span&gt;:19:44 &lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt; +0000

SQLBits &lt;span class=&#34;m&#34;&gt;2017&lt;/span&gt;, &lt;span class=&#34;m&#34;&gt;24&lt;/span&gt; February &lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt;

commit bc5694ee81354a311bb37a457ec5c790ef970f8e &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;HEAD -&amp;gt; master, tag: SQLBits2018&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
Author: Gavin Campbell &amp;lt;gavin@arapaima.uk&amp;gt;
Date:   Tue Feb &lt;span class=&#34;m&#34;&gt;27&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;17&lt;/span&gt;:13:46 &lt;span class=&#34;m&#34;&gt;2018&lt;/span&gt; +0000

    SQLBits Version&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What the tags enable me to do is show two different versions of the &lt;em&gt;same repo&lt;/em&gt; under different urls.&lt;/p&gt;

&lt;p&gt;Before we move onto the next stage, we need to explicitly push our tag(s) to the remote (i.e. GitHub) with the command &lt;code&gt;git push --tags&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;submodules&#34;&gt;Submodules&lt;/h2&gt;

&lt;p&gt;The magic happens through the use of the much-maligned &lt;a href=&#34;https://git-scm.com/docs/git-submodule&#34;&gt;&lt;code&gt;git submodule&lt;/code&gt;&lt;/a&gt; command.&lt;/p&gt;

&lt;p&gt;First, I created an empty repo called &lt;code&gt;presentations&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir presentations &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; presentations
git init&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I clone the repo containing the presentation into a submodule of this empty repo.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD presentations&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$git&lt;/span&gt; submodule add https://github.com/arapaima-uk/database-cd-ssdt-vsts database-cd-sqlbits-2018


Cloning into &lt;span class=&#34;s1&#34;&gt;&amp;#39;/home/gavin/presentations/database-cd-sqlbits-2018&amp;#39;&lt;/span&gt;...
remote: Counting objects: &lt;span class=&#34;m&#34;&gt;233&lt;/span&gt;, &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;.
remote: Compressing objects: &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;% &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;183&lt;/span&gt;/183&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;, &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;.
remote: Total &lt;span class=&#34;m&#34;&gt;233&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;delta &lt;span class=&#34;m&#34;&gt;48&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;, reused &lt;span class=&#34;m&#34;&gt;227&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;delta &lt;span class=&#34;m&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;, pack-reused &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
Receiving objects: &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;% &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;233&lt;/span&gt;/233&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;, &lt;span class=&#34;m&#34;&gt;7&lt;/span&gt;.85 MiB &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;442&lt;/span&gt;.00 KiB/s, &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;.
Resolving deltas: &lt;span class=&#34;m&#34;&gt;100&lt;/span&gt;% &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;48&lt;/span&gt;/48&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;, &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that I am specifying the folder name in which to create the submodule, this will form part of the url of the presentation so it has two purposes; firstly to give the folder a &amp;ldquo;pretty&amp;rdquo; name, and secondly to allow this repo to contain more than one submodule referring to the same source repo, i.e. more than one version of the same presentation, by specifying &lt;code&gt;git submodule add https://github.com/arapaima-uk/database-cd-ssdt-vsts some-other-folder&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The other thing to note is that the clone url of the submodule &lt;em&gt;must&lt;/em&gt; use &lt;code&gt;https://&lt;/code&gt; rather than &lt;code&gt;git://&lt;/code&gt;, or nothing will work. &lt;em&gt;Ask me how I know this sometime.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next, we need to change to the submodule directory, and &lt;code&gt;checkout&lt;/code&gt; the tag we created earlier.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD database-cd-sqlbits-2018&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ git checkout SQLBits2018 
Note: checking out &lt;span class=&#34;s1&#34;&gt;&amp;#39;SQLBits2018&amp;#39;&lt;/span&gt;.

You are in &lt;span class=&#34;s1&#34;&gt;&amp;#39;detached HEAD&amp;#39;&lt;/span&gt; state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
&lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; so &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;now or later&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; by using -b with the checkout &lt;span class=&#34;nb&#34;&gt;command&lt;/span&gt; again. Example:

  git checkout -b &amp;lt;new-branch-name&amp;gt;

HEAD is now at bc5694e... SQLBits Version
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD database-cd-sqlbits-2018&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ git status
HEAD detached at SQLBits2018
nothing to commit, working tree clean&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This has left our repo suffering from the dreaded &amp;ldquo;&lt;a href=&#34;https://www.google.co.uk/search?q=git+detached+head&#34;&gt;detached head&lt;/a&gt;&amp;rdquo; condition, as confirmed above by &lt;code&gt;git status&lt;/code&gt;,  and whilst I would ordinarily regard knowing how to recover from this as a key career-enhancing ability, in this instance we don&amp;rsquo;t need to do anything.&lt;/p&gt;

&lt;p&gt;Without going into a detailed explanation of &lt;code&gt;git submodule&lt;/code&gt;, if we go back to the &lt;code&gt;presentations&lt;/code&gt; root directory (&lt;code&gt;cd ..&lt;/code&gt;), and do &lt;code&gt;git status&lt;/code&gt; again, we can see the following:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD presentations&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ git status
On branch master
Your branch is up-to-date with &lt;span class=&#34;s1&#34;&gt;&amp;#39;origin/master&amp;#39;&lt;/span&gt;.

Changes to be committed:
  &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;use &lt;span class=&#34;s2&#34;&gt;&amp;#34;git reset HEAD &amp;lt;file&amp;gt;...&amp;#34;&lt;/span&gt; to unstage&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;

	modified:   .gitmodules
	new file:   database-cd-sqlbits-2018&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There are no detached heads here, the &lt;code&gt;presentations&lt;/code&gt; repo is still on the &lt;code&gt;master&lt;/code&gt; branch, and we are adding the &lt;code&gt;database-cd-sqlbits-2018&lt;/code&gt; folder as well as modifying the &lt;code&gt;.gitmodules&lt;/code&gt; file, which contains a list of submodules as well as the commits to which they point, in this case the &amp;ldquo;detached head&amp;rdquo; tags. (I think the reason mine says &lt;code&gt;modified&lt;/code&gt; rather than &lt;code&gt;added&lt;/code&gt; is that this isn&amp;rsquo;t the first submodule I&amp;rsquo;ve created.)&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s one more thing we need to do, which is to prevent &lt;a href=&#34;#repo-settings&#34;&gt;Github pages&lt;/a&gt; from trying to render our &lt;a href=&#34;https://revealjs.com&#34;&gt;reveal.js&lt;/a&gt; presentations using &lt;a href=&#34;https://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt;, Github&amp;rsquo;s &amp;ldquo;built-in&amp;rdquo; static site generator. To do this, we just need to create an empty file called &lt;code&gt;.nojekyll&lt;/code&gt; at the root of our repo. I also created a dummy &lt;code&gt;index.html&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD presentations&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$touch&lt;/span&gt; .nojekyll
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD presentations&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;Some Text&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&amp;#34;&lt;/span&gt; &amp;gt; index.html&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;(Yes, this took me a while to figure out. If you don&amp;rsquo;t do this, you will get all sorts of errors reported when you try to publish the site. The index page does get rendered at arapaima.uk/presentations, one day I&amp;rsquo;ll get around to making it prettier, possibly by reading the contents of the submodules and generating it dynamically. However, it isn&amp;rsquo;t linked to anywhere, not even from here!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finally we need to commit these changes in the main repo, and push them to Github.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD presentations&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ git add .
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;gavin@THINKPAD presentations&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;$ git commit -m &lt;span class=&#34;s2&#34;&gt;&amp;#34;added sqlbits presentation&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you haven&amp;rsquo;t done so already, create the &lt;code&gt;presentations&lt;/code&gt; repo in your GitHub account:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/hosting-revealjs-github-pages/createrepo.png&#34; alt=&#34;Creating a Repo in the Github UI&#34; /&gt;
&lt;em&gt;(yes, I already had a &amp;ldquo;&lt;code&gt;presentations&lt;/code&gt;&amp;rdquo;, so this one is called &amp;ldquo;&lt;code&gt;presentation&lt;/code&gt;&amp;rdquo;!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;then add it as a remote and push your changes:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git remote add origin https://github.com/arapaima-uk/presentations.git
git push -u origin master&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can see my &lt;code&gt;presentations&lt;/code&gt; repo on Github &lt;a href=&#34;https://github.com/arapaima-uk/presentations&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;repo-settings&#34;&gt;Repo Settings&lt;/h2&gt;

&lt;p&gt;In the settings for the newly created repo, there is a section &amp;ldquo;Github Pages&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/hosting-revealjs-github-pages/selectmasterbranch.png&#34; alt=&#34;selecting the Github pages branch in the repo settings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After selecting the &lt;code&gt;master&lt;/code&gt; branch and clicking &amp;ldquo;Save&amp;rdquo;, you should see the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/hosting-revealjs-github-pages/branchselected.png&#34; alt=&#34;Github pages publish settings created&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now, because the main site is a Github &lt;a href=&#34;https://help.github.com/articles/user-organization-and-project-pages/&#34;&gt;organisation page&lt;/a&gt; with a custom domain, this project page has been created as a &lt;a href=&#34;https://help.github.com/articles/custom-domain-redirects-for-github-pages-sites/&#34;&gt;subdirectory of the main site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If all is well, and you&amp;rsquo;ve got to here, you should be able to see your presentation on &lt;a href=&#34;http://arapaima.uk/presentations/database-cd-sqlbits-2018/&#34;&gt;http://arapaima.uk/presentations/database-cd-sqlbits-2018/&lt;/a&gt;, substituting your domain for &lt;a href=&#34;http://arapaima.uk&#34;&gt;arapaima.uk&lt;/a&gt; and your presentation title for &lt;code&gt;database-cd-sqlbits-2018&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a list of all the talks I&amp;rsquo;ve done, of which the top few have slide decks linked in this manner, &lt;a href=&#34;http://arapaima.uk/fixed/speaking/#past&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;further-reading&#34;&gt;Further reading&lt;/h2&gt;

&lt;p&gt;These are some of the articles I found useful when putting this all together:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://jpmoral.com/blogging/2015/07/29/hosting-revealjs-slides-on-jekyll.html&#34;&gt;Hosting Reveal.js Slide Decks on a Jekyll-generated blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.chenhuijing.com/blog/revealjs-and-github-pages/&#34;&gt;Reveal.js + GitHub Pages: when developers give talks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twoguysarguing.wordpress.com/2010/11/14/tie-git-submodules-to-a-particular-commit-or-branch/&#34;&gt;Tie Git Submodules to a Particular Commit or Branch &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://annaken.github.io/hosting-revealjs-presentation-github-pages&#34;&gt;Hosting a reveal.js presentation on github pages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Expose VSTS secrets as environment variables with this one weird trick...</title>
          <link>http://arapaima.uk/post/2017-10-23-vsts-secret-environment-variables/</link>
          <pubDate>Mon, 23 Oct 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-10-23-vsts-secret-environment-variables/</guid>
          <description>

&lt;h2 id=&#34;config-as-environment-variables&#34;&gt;Config as environment variables&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m a big fan of the &lt;a href=&#34;https://12factor.net/&#34;&gt;Twelve-Factor App&lt;/a&gt; &amp;ldquo;methodology&amp;rdquo;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; for building and deploying applications, and whilst much of it is geared towards web apps in Heroku-esque environments, I think the principles - or &amp;ldquo;factors&amp;rdquo; - are well worth bearing in mind when considering the delivery of other types of application.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://12factor.net/config&#34;&gt;Factor 3 of the 12&lt;/a&gt; reads as follows&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Resource handles to the database, Memcached, and other backing services&lt;/li&gt;
&lt;li&gt;Credentials to external services such as Amazon S3 or Twitter&lt;/li&gt;
&lt;li&gt;Per-deploy values such as the canonical hostname for the deploy&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are a number of benefits to this approach, the main ones I can think of are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The obvious one about not having credentials stored in source control. Nobody does this anymore, right?&lt;/li&gt;
&lt;li&gt;If environment specific information such as server names are stored with the source control, then changes in the infrastructure will result in new commits in the source repo, meaning that the commit history will no longer merely &amp;ldquo;tell the story&amp;rdquo; of the application, but will also contain numerous sub-plots regarding the infrastructure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;the-downside&#34;&gt;The downside&lt;/h3&gt;

&lt;p&gt;There is some debate over whether environment variables are really the best place for secret information, and there are definitely customers for whom this approach would be considered too high risk. However, I wouldn&amp;rsquo;t have thought these included the customers where the credentials are currently stored with the application source code!&lt;/p&gt;

&lt;p&gt;The main alternatives generally revolve around storing credentials somewhere where the infrastructure automation tools - Ansible, etc. - can see them and using these tools to deploy a file which the applications can read.&lt;/p&gt;

&lt;h2 id=&#34;never-mind-the-downside-on-with-the-weird-trick&#34;&gt;Never mind the downside, on with the weird trick&amp;hellip;&lt;/h2&gt;

&lt;p&gt;I used the following example of the &amp;ldquo;config as environment variables&amp;rdquo; approach in a recent talk about SSDT and VSTS, using a Powershell Script to read config values from environment variables and deploy a dacpac to a SQL Azure database using SQL Authentication.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Powershell&#34; data-lang=&#34;Powershell&#34;&gt;$serverName=$env:Chinook_ServerName
$dbName = $env:Chinook_DatabaseName
$dbUser = $env:Chinook_DbUser
$dbPassword = $env:Chinook_DbPassword

$dacFxDll=&amp;#39;C:\Program Files (x86)\Microsoft SQL Server\140\DAC\bin\Microsoft.SqlServer.Dac.dll&amp;#39;

Add-Type -Path $dacFxDll
$dacServices = New-Object Microsoft.SqlServer.Dac.DacServices &amp;#34;server=$serverName;User ID=$dbUser;Password=$dbPassword;&amp;#34;

$dacpacPath=Join-Path -Path $PSScriptRoot -ChildPath &amp;#34;\bin\Debug\ChinookDb.dacpac&amp;#34;
$publishProfilePath = Join-Path -Path $PSScriptRoot -ChildPath &amp;#34;CommonSettings.publish.xml&amp;#34;

$dacpac = [Microsoft.SqlServer.Dac.DacPackage]::Load($dacpacPath)
$dacProfile = [Microsoft.SqlServer.Dac.DacProfile]::Load($publishProfilePath)

$dacServices.Deploy($dacpac, $dbName, $true, $dacProfile.DeployOptions )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this script does, in brief, is load the server name, database name, and credentials from environment variables, and then deploy a &lt;code&gt;.dacpac&lt;/code&gt; using this information in addition to a publish profile(&lt;code&gt;CommonSettings.publish.xml&lt;/code&gt;) that defines some common - to &lt;em&gt;all&lt;/em&gt; environments - deployment configuration.&lt;/p&gt;

&lt;p&gt;The main advantage of this approach is that the &lt;em&gt;same&lt;/em&gt; deployment script can be used without modification in &lt;em&gt;all&lt;/em&gt; environments, from the developer&amp;rsquo;s desktop through the various testing environments and on to UAT and Production.&lt;/p&gt;

&lt;p&gt;So, for deployments from the desktop or other unmanaged environments, it is easy to specify these values by setting environment variables.&lt;/p&gt;

&lt;p&gt;However, in VSTS, things are a little more complicated. It&amp;rsquo;s possible to use &lt;a href=&#34;https://docs.microsoft.com/en-us/vsts/build-release/concepts/agents/pools-queues#creating-agent-pools-and-queues&#34;&gt;private agent queues&lt;/a&gt; to allocate specific build agents - which could have these variables set in advance -  to specific environments, but what if we just want to use the &lt;a href=&#34;https://docs.microsoft.com/en-us/vsts/build-release/concepts/agents/hosted#use-a-hosted-agent&#34;&gt;hosted queue&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/vsts/build-release/concepts/releases/&#34;&gt;VSTS Release Management&lt;/a&gt; allows us to specify variables for each environment in our &lt;a href=&#34;https://docs.microsoft.com/en-us/vsts/build-release/concepts/definitions/release/&#34;&gt;Release Definition&lt;/a&gt;. It&amp;rsquo;s fairly common to see these used as parameters to deployment tasks, but what is possibly less obvious from this interface is that these values are surfaced as &lt;em&gt;environment variables&lt;/em&gt; in the build process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-secret-vars/variablesinreleasedefinition.png&#34; alt=&#34;Variables pane in VSTS Release Management&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This means that our Powershell script above can still work unmodified - except, that is, for the password. This is masked in the screenshot  as it is defined as a &amp;ldquo;secret&amp;rdquo; variable. Secret variables are &lt;a href=&#34;https://docs.microsoft.com/en-us/vsts/build-release/concepts/definitions/build/variables?tabs=batch#secret-variables&#34;&gt;not exposed as environment variables&lt;/a&gt;, but can only be passed as arguments to our build steps. What muddies the water slightly is that within living memory &lt;a href=&#34;https://github.com/Microsoft/vsts-agent/issues/145&#34;&gt;secret variables &lt;em&gt;were&lt;/em&gt; exposed as environment variables&lt;/a&gt;, but this behaviour was &amp;ldquo;fixed&amp;rdquo; some time in 2016.&lt;/p&gt;

&lt;h3 id=&#34;finally-the-weird-trick&#34;&gt;Finally, the weird trick&lt;/h3&gt;

&lt;p&gt;In the above example, there is an empty environment variable &lt;code&gt;Chinook_DbPassword&lt;/code&gt; for each environment, and a corresponding &lt;em&gt;secret&lt;/em&gt; variable that contains the actual password.&lt;/p&gt;

&lt;p&gt;We need to add an additional task to our release definition to read the secret variable and surface it as an environment variable. This can be done through the &lt;a href=&#34;https://github.com/Microsoft/vsts-tasks/blob/master/docs/authoring/commands.md&#34;&gt;VSTS logging commands&lt;/a&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, which are worth checking out as they enable a number of &amp;ldquo;weird tricks&amp;rdquo; in addition to this particular one.&lt;/p&gt;

&lt;p&gt;As per the documentation, Logging Commands are invoked by writing the command to standard output, which in the case of PowerShell is done via &lt;code&gt;Write-Host&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-secret-vars/PowerShellStepToReadSecretVariable.png&#34; alt=&#34;Release Definition showing PowerShell step to decrypt secret variable&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In accordance with the &amp;ldquo;rules&amp;rdquo;, we pass the secret variable as an argument to the script, then use the &lt;code&gt;##vso[task.setvariable]&lt;/code&gt; command to set the value of the non-secret environment variable to the value of the argument.&lt;/p&gt;

&lt;p&gt;In the logs for the release, we can see the non-secret variables being set, with &lt;code&gt;Chinook_dbPassword&lt;/code&gt; set to blank (&lt;code&gt;[]&lt;/code&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-10-13T12:00:12.1607197Z Environment variables available are below.  
...
...
    [AGENT_HOMEDIRECTORY] --&amp;gt; [C:\LR\mms\Services\Mms\Provisioner\TaskAgent\agents\2.123.0]
...    
...
    [CHINOOK_DATABASENAME] --&amp;gt; [Chinook]
    [CHINOOK_DBPASSWORD] --&amp;gt; []
    [CHINOOK_DBUSER] --&amp;gt; [arapaima]
    [CHINOOK_SERVERNAME] --&amp;gt; [vstsdemochinook.database.windows.net]
...
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we get to the logs for the &amp;ldquo;Read Env Var&amp;rdquo; step:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-10-13T12:00:14.5784670Z ##[section]Starting: Read Env Var
2017-10-13T12:00:14.5954676Z ==============================================================================
2017-10-13T12:00:14.5954676Z Task         : PowerShell
2017-10-13T12:00:14.5954676Z Description  : Run a PowerShell script
2017-10-13T12:00:14.5954676Z Version      : 1.2.3
2017-10-13T12:00:14.5954676Z Author       : Microsoft Corporation
2017-10-13T12:00:14.5954676Z Help         : [More Information]
2017-10-13T12:00:14.5954676Z ==============================================================================
2017-10-13T12:00:14.6704451Z ##[command]. &#39;d:\a\_temp\2fa5955a-1363-464a-bed7-aed0cbea2c96.ps1&#39; ********
2017-10-13T12:00:15.5054467Z ##[section]Finishing: Read Env Var
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;we can see that the secret variable is passed as a parameter to the inline script, but masked with asterisks in the release logs.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;Deploy Dacpac&amp;rdquo; step contains a single action, namely running the same Powershell script as was used in every other environment:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2017-10-13T12:01:30.0026563Z ##[section]Starting: Deploy Dacpac
2017-10-13T12:01:30.0036563Z ==============================================================================
2017-10-13T12:01:30.0036563Z Task         : PowerShell
2017-10-13T12:01:30.0036563Z Description  : Run a PowerShell script
2017-10-13T12:01:30.0036563Z Version      : 1.2.3
2017-10-13T12:01:30.0036563Z Author       : Microsoft Corporation
2017-10-13T12:01:30.0036563Z Help         : [More Information]
2017-10-13T12:01:30.0036563Z ==============================================================================
2017-10-13T12:01:30.0076572Z ##[command]. &#39;d:\a\r1\a\ChinookDb-CI\BuildOutput\Deploy.ps1&#39; 
2017-10-13T12:02:10.3410313Z ##[section]Finishing: Deploy Dacpac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This isn&amp;rsquo;t restricted to Release Definitions, the same technique will work in Build Definitions too. This is possibly a less common scenario - as &lt;em&gt;builds&lt;/em&gt; shouldn&amp;rsquo;t normally contain environment-specific information - except in cases like database deployment where it&amp;rsquo;s generally necessary to deploy the database &lt;a href=&#34;https://stackoverflow.com/questions/39443317/can-you-run-sql-unit-tests-tsqlt-during-build-process-in-vsts&#34;&gt;&lt;em&gt;somewhere&lt;/em&gt;&lt;/a&gt; before we can do any automated testing.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Their words, not mine!
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;I can&amp;rsquo;t find this page on docs.microsoft.com, so the link is to the &lt;a href=&#34;https://github.com/Microsoft/vsts-tasks/blob/master/docs/authoring/commands.md&#34;&gt;Github source&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Combining tSQLt mocks with Visual Studio SQL Server Unit Tests</title>
          <link>http://arapaima.uk/post/2017-09-13-visual-studio-database-unit-tests-tsqlt/</link>
          <pubDate>Wed, 13 Sep 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-09-13-visual-studio-database-unit-tests-tsqlt/</guid>
          <description>

&lt;p&gt;This came up in a question after a recent talk about database unit testing; I&amp;rsquo;ve done something similar on a client project in the past, and it was in my &amp;ldquo;old&amp;rdquo; talk about testing. I thought I&amp;rsquo;d write it down here in case it&amp;rsquo;s useful to anyone, not least the person who was asking the question.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;.zip&lt;/code&gt; file of the complete solution can be downloaded from &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/KebabTestDemo.zip&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For many years, Visual Studio Database Projects - in SSDT as well as in its predecessors - have included an additional template for generating SQL Server Unit Tests.&lt;/p&gt;

&lt;h2 id=&#34;sql-server-unit-tests&#34;&gt;SQL Server Unit Tests&lt;/h2&gt;

&lt;p&gt;SQL Server Unit Tests live in a SQL Server Unit Test &lt;em&gt;Class&lt;/em&gt;, which is contained in an ordinary (.NET) Unit Test Project.&lt;/p&gt;

&lt;p&gt;There is some additional configuration scoped to the Test &lt;em&gt;Project&lt;/em&gt;, namely the connection string(s) that will be used to execute the tests, and optionally the name of a database project to deploy before every test run.&lt;/p&gt;

&lt;p&gt;The Test Class is an ordinary test class with some boilerplate code supplied as well as a designer for creating SQL Server Unit Tests.&lt;/p&gt;

&lt;p&gt;The advantages of using Visual Studio to create our SQL Server Unit Tests are that the ouputs of the test are produced in &lt;code&gt;.trx&lt;/code&gt; format, which is well understood by many CI tools, not least the ones from Microsoft, and that the tests can be run by the test runner built into Visual Studio.&lt;/p&gt;

&lt;h3 id=&#34;the-sql-server-unit-test-designer&#34;&gt;The SQL Server Unit Test Designer&lt;/h3&gt;

&lt;p&gt;The test designer supports up to three Transact SQL scripts per test, called Pre-Test, Test, and Post-Test, which are run in the order you would expect. There is also the option to create class-scoped scripts for Test Initialize and Test Cleanup, which are run before and after every individual test in the class.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/unitTestDesigner.png&#34; alt=&#34;The SQL Server Unit Test Designer&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For each of these scripts, there is the option to add test assertions, which are referred to  &amp;ldquo;Test Conditions&amp;rdquo; in this framework. The built-in test conditions are not particularly flexible, but writing custom test conditions is a topic for another day.&lt;/p&gt;

&lt;p&gt;At the time of writing, it is almost exactly ten years since the inventor of NUnit (and of other things) decided that common SetUp and TearDown methods &lt;a href=&#34;http://jamesnewkirk.typepad.com/posts/2007/09/why-you-should-.html&#34;&gt;probably weren&amp;rsquo;t such a good idea after all&lt;/a&gt;, mainly as they make it difficult to see what a test is actually &lt;em&gt;doing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is complicated further in SQL Server Unit Tests, as the Pre-Test and Post-Test scripts don&amp;rsquo;t use the same connection as the main Test script, which can lead to even more unexpected results.&lt;/p&gt;

&lt;p&gt;So, this example won&amp;rsquo;t make use of the Pre-Test and Post-Test scripts, or of their class-scoped equivalents.&lt;/p&gt;

&lt;p&gt;Finally, there is no support for mocks or fakes of any kind in the Visual Studio SQL Server unit testing framework.&lt;/p&gt;

&lt;p&gt;Fortunately, there is another testing framework that provides just such support, namely &lt;a href=&#34;http://tsqlt.org/&#34;&gt;tSQLt&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;tsqlt&#34;&gt;tSQLt&lt;/h3&gt;

&lt;p&gt;In the .NET/Java/whatever worlds, after deciding on a Unit Test Framework to use - NUnit, xUnit, MSTest, etc. - we are still faced with a further decision regarding what mocking framework to use, for instance Moq, NSub, Rhino, the Microsoft one that nobody&amp;rsquo;s heard of because it&amp;rsquo;s only in VS Enterprise, etc. etc.&lt;/p&gt;

&lt;p&gt;In the case of tSQLt, the Unit Test framework and the mocking framework are bundled together into a single package.&lt;/p&gt;

&lt;p&gt;However, this doesn&amp;rsquo;t mean that these components can&amp;rsquo;t be &lt;em&gt;used&lt;/em&gt; in isolation from one another.&lt;/p&gt;

&lt;h2 id=&#34;crossing-the-streams&#34;&gt;Crossing the streams&lt;/h2&gt;

&lt;p&gt;The present example will demonstrate the use of the mocking facilities of tSQLt in conjunction with the test designer and test runner built into Visual Studio.&lt;/p&gt;

&lt;p&gt;There are a few ways of getting the tSQLt objects deployed to where they are needed for testing, the way I use most often is basically &lt;a href=&#34;https://the.agilesql.club/Blog/Ed-Elliott/AdventureWorksCI-Step5-Adding-tSQLt-Dacpac-To-The-Solution&#34;&gt;this one&lt;/a&gt;, whereby we create a &lt;code&gt;.dacpac&lt;/code&gt; of just the tSQLt objects (or use one we made earlier!), and create a second database project with a Database Reference of type &amp;ldquo;Same Database&amp;rdquo; to the project we are trying to test, and a reference to our tSQLt &lt;code&gt;.dacpac&lt;/code&gt;. The &lt;code&gt;.dacpac&lt;/code&gt; file needs to be somewhere in our source folders, as it will be added by path. We also need a reference to the master database, as this is required to build the tSQLt project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/SolutionExplorerView.png&#34; alt=&#34;Solution Explorer View showing tsqlt projects&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the above illustration, KebabTestDemo is the application we are testing, KebabTestDemo.tSQLt is the database project that contains &lt;em&gt;only&lt;/em&gt; the references to our tSQLt dacpac and to master, and KebabTestDemo.Tests is the Unit Test project that contains our SQL Server Unit Test.&lt;/p&gt;

&lt;p&gt;In the &amp;ldquo;SQL Server Test Configuration&amp;rdquo; dialog, we specify the connection string to use for runnng our tests. This information is stored in the &lt;code&gt;app.config&lt;/code&gt; for the test assembly, meaning it is scoped to the &lt;em&gt;project&lt;/em&gt; rather than to the individual test &lt;em&gt;classes&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/SQLServerTestConfiguration.png&#34; alt=&#34;The SQL Server Test Configuration dialog&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This dialog also allows us to specify that we want to deploy a database project at the start of every test run, so that all of our latest changes get included. The keen-eyed will notice, however, that we can only specify &lt;em&gt;one&lt;/em&gt; project here, whereas we have &lt;em&gt;two&lt;/em&gt; database projects in our solution. Normally I&amp;rsquo;d just leave this blank and try to remember to deploy my updated project before every test run in Visual Studio, and hope that the CI Server &amp;ldquo;remembers&amp;rdquo; to deploy its projects before its own test runs.&lt;/p&gt;

&lt;p&gt;However, there is a solution, outlined in an &lt;a href=&#34;https://blogs.msdn.microsoft.com/bahill/2009/07/31/deploying-composite-projects-through-a-database-unit-test-run/&#34;&gt;MSDN blog post from 2009&lt;/a&gt; that allows us to take advantage of this &amp;ldquo;automatic deployment&amp;rdquo; feature from Visual Studio.&lt;/p&gt;

&lt;p&gt;In short, we subclass the &lt;code&gt;SqlDatabaseTestService&lt;/code&gt; class used in &lt;code&gt;SqlDatabaseSetup.cs&lt;/code&gt; to allow us to deploy not one, but two projects from the &lt;code&gt;InitializeAssembly&lt;/code&gt; method.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;KebabDbTestService&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SqlDatabaseTestService&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DeployCompositeProject&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;DeployDatabaseProject&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;@&amp;#34;..\..\..\KebabTestDemo\KebabTestDemo.sqlproj&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Release&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;System.Data.SqlClient&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GetConnectionString&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;DeployDatabaseProject&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;@&amp;#34;..\..\..\KebabTestDemo.tSQLt\KebabTestDemo.tSQLt.sqlproj&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Release&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;System.Data.SqlClient&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GetConnectionString&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;());&lt;/span&gt;

    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;private&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;GetConnectionString&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;kt&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SqlUnitTestingSection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ConfigurationManager&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GetSection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlUnitTesting&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;config&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ExecutionContext&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ConnectionString&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that this pays no attention to the &lt;code&gt;app.config&lt;/code&gt; setting that tells us what projects to deploy, so caution is advised!&lt;/p&gt;

&lt;p&gt;We then call our new code from the &lt;code&gt;InitializeAssembly&lt;/code&gt; method in &lt;code&gt;SqlDatabaseSetup&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// Setup the test database based on setting in the
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;// configuration file
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;//SqlDatabaseTestClass.TestService.DeployDatabaseProject();
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;//SqlDatabaseTestClass.TestService.GenerateData();
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;kt&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;service&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KebabDbTestService&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;service&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DeployCompositeProject&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
(the commented out code is the previous contents of this method)&lt;/p&gt;

&lt;h3 id=&#34;transactions&#34;&gt;Transactions&lt;/h3&gt;

&lt;p&gt;One of the features of tSQLt is that all the procedures for running unit tests wrap every individual test in a transaction which is rolled back at the end of the test, meaning that the database is in the same state at the end of the test as at the beginning. This is unquestionably a &lt;em&gt;good thing&lt;/em&gt;, as it means that the tests are all independent of one another, and that we don&amp;rsquo;t need to think about test &amp;ldquo;teardown&amp;rdquo; actions.&lt;/p&gt;

&lt;p&gt;In the case of Visual Studio unit tests, we need to add this support manually. There are a few ways of doing this &lt;a href=&#34;https://msdn.microsoft.com/en-US/library/jj851217.aspx&#34;&gt;documented on MSDN&lt;/a&gt;, of which I&amp;rsquo;ll consider two.&lt;/p&gt;

&lt;p&gt;The first is to insert &lt;code&gt;BEGIN TRANSACTION&lt;/code&gt; and &lt;code&gt;ROLLBACK TRANSACTION&lt;/code&gt; at the beginning and end of every test script. Whilst this is effective, you need to remember to do it every time. My preferred method requires &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/jj851217.aspx#Anchor_2&#34;&gt;further doctoring of the C# code behind the designer&lt;/a&gt; so that every test is wrapped in a &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/system.transactions.transactionscope.aspx&#34;&gt;&lt;code&gt;TransactionScope&lt;/code&gt;&lt;/a&gt;. The only thing to remember here is that the Distributed Transaction Co-ordinator, better known as MSDTC, must be running on the machine where the test is executed, whether this is on your desktop or on a build server.&lt;/p&gt;

&lt;p&gt;The only changes we make are the ones highlighted below; we add a reference to &lt;code&gt;System.Transactions&lt;/code&gt; and a member variable of type &lt;code&gt;System.Transactions.TransactionScope&lt;/code&gt;. We then initialise this variable in &lt;code&gt;TestInitialize()&lt;/code&gt; and call its &lt;code&gt;Dispose()&lt;/code&gt; method in &lt;code&gt;TestCleanup()&lt;/code&gt;, which will throw away the transaction without committing it.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;k&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;System.Transactions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;namespace&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;KebabTestDemo.Tests&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;    [TestClass()]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;KebabOrderLineTests&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SqlDatabaseTestClass&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;hl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;TransactionScope&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;        &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KebabOrderLineTests&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
            &lt;span class=&#34;n&#34;&gt;InitializeComponent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;        [TestInitialize()]&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TestInitialize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;hl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;_t&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TransactionScope&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;            &lt;span class=&#34;k&#34;&gt;base&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InitializeTest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;        [TestCleanup()]&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TestCleanup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
            &lt;span class=&#34;k&#34;&gt;base&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CleanupTest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;span class=&#34;hl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;_t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dispose&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;        &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now when we run our tests, each individual test will be wrapped in a transaction, which will be disposed of (i.e. rolled back) at the end of the test.&lt;/p&gt;

&lt;h3 id=&#34;the-test-script&#34;&gt;The Test Script&lt;/h3&gt;

&lt;p&gt;The test script now consists of only three lines; &lt;a href=&#34;http://tsqlt.org/user-guide/isolating-dependencies/faketable/&#34;&gt;faking the table&lt;/a&gt; used in the test, calling the procedure, and selecting the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/VSUnitTestWithTsqltMocks.png&#34; alt=&#34;The completed Visual Studio Unit Test with tSQLt Mocks&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The results will be processed by the &amp;ldquo;Test Condition&amp;rdquo; at the bottom of the picture, this is a &amp;ldquo;Data Checksum&amp;rdquo; condition, which is the only way to validate a multi-column, multi-row result using the built-in test conditions.&lt;/p&gt;

&lt;p&gt;The checksum is configured using the following dialog; we have to select a database connection to use (in this case, it doesn&amp;rsquo;t really matter what that connection is), followed by a query that will return the same result set (including column names) as we expect the result set of the test to return. We then click &amp;ldquo;Retrieve&amp;rdquo; to execute the query, retrieve the results, and populate the checksum value (in this case &lt;code&gt;-1371852473&lt;/code&gt;, visible in the screenshot above)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/DataChecksumConfiguration.png&#34; alt=&#34;The Data Checksum Test Configuration Dialog&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;running-the-test&#34;&gt;Running the Test&lt;/h2&gt;

&lt;p&gt;Having got to here, we are ready to run our test from the Visual Studio Test Explorer. This will re-deploy our project(s) and run the test, wrapped in a &lt;code&gt;TransactionScope&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/PassingTest.png&#34; alt=&#34;ScreenShot of passing test&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is a bit fiddly to set up, and even more fiddly to set up on a CI Server. However if you have some reason why you want or need to use the built-in testing facilities in Visual Studio, then hopefully this article has demonstrated a way to take advantage of the tSQLt mocking framework.&lt;/p&gt;

&lt;p&gt;If you &lt;a href=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/tSQlt-VsTest/KebabTestDemo.zip&#34;&gt;download the sample project&lt;/a&gt;, then the test should build and run the first time, once you set the connection string in the &amp;ldquo;SQL Server Test Configuration&amp;rdquo; dialog.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Automatically provisioning a brand new environment for every feature branch using VSTS and AzureRM</title>
          <link>http://arapaima.uk/post/2017-09-07-vsts-azurerm-dynamic-environment-provisioning/</link>
          <pubDate>Thu, 07 Sep 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-09-07-vsts-azurerm-dynamic-environment-provisioning/</guid>
          <description>

&lt;p&gt;It&amp;rsquo;s fairly uncontentious to suggest that, all else being equal, providing each developer with an individual &amp;ldquo;sandbox&amp;rdquo;, or private development environment, is a &lt;a href=&#34;http://www.agiledata.org/essays/sandboxes.html&#34;&gt;worthwhile endeavour&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Often, these can be provisioned on the developers individual desktops, but when the application involves PaaS services such as databases, message queues, and other cloud-based services, things become more complicated. It&amp;rsquo;s generally possible to emulate &lt;em&gt;most&lt;/em&gt; things on the desktop, but there are often small gaps in this emulation, not least in the communication and authentication protocols that link the services together.&lt;/p&gt;

&lt;p&gt;The rest of this article will discuss how to use &lt;a href=&#34;https://www.visualstudio.com/team-services/&#34;&gt;Visual Studio Team Services&lt;/a&gt; in conjunction with &lt;a href=&#34;https://azure.microsoft.com/en-gb/features/resource-manager/&#34;&gt;Azure Resource Manger&lt;/a&gt; (henceforward AzureRM) templates to automatically provision a new environment for every branch created in source control, and automatically destroy the environment using a service hook to &lt;a href=&#34;https://azure.microsoft.com/en-gb/services/automation/&#34;&gt;Azure Automation&lt;/a&gt; when the branch is merged to master and deleted.&lt;/p&gt;

&lt;p&gt;I stumbled on this technique whilst working on a proof of concept involving the data-related Azure services, such as Data Factory, Data Lake, and Azure SQL Data Warehouse, but the present example consists of a serverless &lt;a href=&#34;https://azure.microsoft.com/en-gb/services/functions/&#34;&gt;Azure Function&lt;/a&gt; that writes to a &lt;a href=&#34;https://azure.microsoft.com/en-gb/services/cosmos-db/&#34;&gt;Cosmos DB&lt;/a&gt; PaaS Database.&lt;/p&gt;

&lt;p&gt;This technique should generalise to most Azure PaaS services, and yes, I&amp;rsquo;m sure you can do similar things with AWS and friends but I haven&amp;rsquo;t had cause to think about them for the time being.&lt;/p&gt;

&lt;h3 id=&#34;the-application-code&#34;&gt;The application code&lt;/h3&gt;

&lt;p&gt;The project I&amp;rsquo;m using for this example can be downloaded from &lt;a href=&#34;https://github.com/arapaima-uk/KebabAzureRmDemo&#34;&gt;this Github repo&lt;/a&gt;. The Github version of the code is mirrored from the version in VSTS using the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=nkdagility.gittasks&#34;&gt;Git Tasks&lt;/a&gt; extension for VSTS&lt;/p&gt;

&lt;p&gt;The repo contains a Visual Studio (2017) solution with four projects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FavouriteKebab - this is the Azure Function App&lt;/li&gt;
&lt;li&gt;KebabDbResourceGroup - this is the AzureRM template that defines our infrastructure&lt;/li&gt;
&lt;li&gt;KebabTests - contains a single test written with the MSTest framework&lt;/li&gt;
&lt;li&gt;HelperScripts - a Powershell Script project containing a single script used in the build pipeline&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/solutionexplorerview.PNG&#34; alt=&#34;Solution Explorer View&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-next-instagram&#34;&gt;The next Instagram?&lt;/h3&gt;

&lt;p&gt;The details of what the app actually &lt;em&gt;does&lt;/em&gt; aren&amp;rsquo;t particularly important for this example, but in brief it is expecting a JSON structure:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bill&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;#34;favouriteKebab&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Adana&amp;#34;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;in the body of a &lt;code&gt;POST&lt;/code&gt; message, which it will then store in the Cosmos DB database.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span class=&#34;na&#34;&gt;[FunctionName(&amp;#34;KebabPost&amp;#34;)]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HttpResponseMessage&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Run&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;[HttpTrigger(AuthorizationLevel.Anonymous, &amp;#34;post&amp;#34;)]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;HttpRequestMessage&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;[DocumentDB(&amp;#34;kebabDb&amp;#34;, &amp;#34;kebabPreferences&amp;#34;,
&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;ConnectionStringSetting = &amp;#34;DB_CONNECTION&amp;#34;, CreateIfNotExists =true)]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;dynamic&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nameAndKebabType&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;n&#34;&gt;TraceWriter&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;C# HTTP trigger function processed a request.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;// Get request body
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;kt&#34;&gt;dynamic&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReadAsAsync&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;object&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

    &lt;span class=&#34;c1&#34;&gt;// Set name to query string or body data
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;nameAndKebabType&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;?.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;favouriteKebab&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;?.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;favouriteKebab&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;?.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;null&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;||&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;?.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;favouriteKebab&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;null&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CreateResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HttpStatusCode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;BadRequest&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
            &lt;span class=&#34;s&#34;&gt;&amp;#34;Name and kebab are both required!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

    &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;req&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CreateResponse&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HttpStatusCode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OK&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The name of the DocumentDB database and collection will be the same in every environment, so I&amp;rsquo;ve just hardcoded them in the file (&lt;code&gt;&amp;quot;kebabDB&amp;quot;&lt;/code&gt; and &lt;code&gt;&amp;quot;kebabPreferences&amp;quot;&lt;/code&gt;). The other thing to note is that I am passing &lt;code&gt;CreateIfNotExists=true&lt;/code&gt; in the binding options for the DocumentDB collection; it turns out to be non-trivial to create the database and the collection from an AzureRM template, so we let the app create them at runtime if required.&lt;/p&gt;

&lt;p&gt;The test project contains a single test, which exercises the behaviour described above:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-csharp&#34; data-lang=&#34;csharp&#34;&gt;&lt;span class=&#34;na&#34;&gt;[TestMethod]&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;KebabPost_ValidInput_StoresValuesInDb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;kt&#34;&gt;string&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;{ &amp;#39;name&amp;#39;:&amp;#39;Barry&amp;#39; , &amp;#39;favouriteKebab&amp;#39;:&amp;#39;Kofte&amp;#39; }&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HttpClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;kt&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PostAsync&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;functionUrl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StringContent&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Encoding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;UTF8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;application/json&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Result&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
    
&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DocumentClient&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;docDbUri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;docDbKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;IQueryable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;KebabPrefs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kebabPrefsQuery&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;client&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CreateDocumentQuery&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;KebabPrefs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;UriFactory&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CreateDocumentCollectionUri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;kebabDb&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;kebabPreferences&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Where&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;kp&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Barry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;

    &lt;span class=&#34;n&#34;&gt;Assert&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AreEqual&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Kofte&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;kebabPrefsQuery&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ToList&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;First&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;().&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;favouriteKebab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;


    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Unit Testing enthusiasts will have noticed that this isn&amp;rsquo;t really a Unit Test at all; there are no fakes or mocks, we are using the &amp;ldquo;real&amp;rdquo; dependencies for everything.&lt;/p&gt;

&lt;p&gt;If our Azure function were a bit bigger, it would be worth splitting out the &amp;ldquo;core&amp;rdquo; functionality into a separate assembly that &lt;em&gt;could&lt;/em&gt; be meaningfully unit tested, but in the interests of brevity I&amp;rsquo;ve not done that here.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Integration&lt;/em&gt; tests of this kind are pretty common in PaaS projects, and a glance at the &lt;a href=&#34;https://github.com/arapaima-uk/KebabAzureRmDemo/blob/master/KebabTests/KebabTests.cs&#34;&gt;full source code of the test project &lt;/a&gt; will reveal many of the common &amp;ldquo;characteristics&amp;rdquo; of coded integration tests; acres of boilerplate code to read config files and set up the initial conditions, POCOs not present in the main application whose only purpose is to hold data for our test, etc. etc. Still, all of this fragility and maintenance difficulty is probably a price worth paying for increased confidence in our deployment pipeline.&lt;/p&gt;

&lt;h2 id=&#34;the-azurerm-template&#34;&gt;The AzureRM Template&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/arapaima-uk/KebabAzureRmDemo/blob/master/KebabDbResourceGroup/azuredeploy.json&#34;&gt;full template&lt;/a&gt; is a bit too long to reproduce here, so I&amp;rsquo;ll call out some edited highlights.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;parameters&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;database_account_name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;defaultValue&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[concat(&amp;#39;kebabdac&amp;#39;, uniqueString(resourceGroup().id))]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;hostingPlanName&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;defaultValue&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[concat(&amp;#39;kebabhp&amp;#39;, uniqueString(resourceGroup().id))]&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;functionAppName&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;defaultValue&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[concat(&amp;#39;kebabApp&amp;#39;, uniqueString(resourceGroup().id))]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;storageAccountName&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;defaultValue&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[concat(&amp;#39;kebabstor&amp;#39;, uniqueString(resourceGroup().id))]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The thing to note is that the template takes four parameters, each of which has a default value.&lt;/p&gt;

&lt;p&gt;Each of the defaults consists of a prefix that identifies the resource type, combined with the result of the function &lt;code&gt;uniqueString(resourceGroup().id)&lt;/code&gt;. These unique strings are required as there are some objects in Azure (storage accounts or web apps for example) that require &lt;em&gt;globally&lt;/em&gt; unique names that can be incorporated into a url of the form &lt;code&gt;https://myapp.azurewebsites.net&lt;/code&gt; or similar. I can never remember which objects do require unique names and which ones don&amp;rsquo;t, so I tend to use &lt;code&gt;uniqueString()&lt;/code&gt; for everything.&lt;/p&gt;

&lt;p&gt;The template goes on to create a storage account, a Cosmos DB account, a Function App, and all the supporting bits these require. These end up with some pretty funky names thanks to the use of &lt;code&gt;uniqueString()&lt;/code&gt;, but since you never really need to type these names anywhere this is more of an aesthetic consideration than a practical problem:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/resources.png&#34; alt=&#34;Azure resource names generated with uniqueString&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At the end of the template is an &lt;code&gt;outputs&lt;/code&gt; section:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;outputs&amp;#34;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;functionAppName&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[parameters(&amp;#39;functionAppName&amp;#39;)]&amp;#34;&lt;/span&gt;

    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;docDbUri&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[Concat(&amp;#39;https://&amp;#39;,parameters(&amp;#39;database_account_name&amp;#39;),&amp;#39;.documents.azure.com:443&amp;#39;)]&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
    &lt;span class=&#34;nt&#34;&gt;&amp;#34;docDbKey&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;string&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
      &lt;span class=&#34;nt&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[listKeys(resourceId(&amp;#39;Microsoft.DocumentDb/databaseAccounts&amp;#39;, parameters(&amp;#39;database_account_name&amp;#39;)), &amp;#39;2015-04-08&amp;#39;).primaryMasterKey]&amp;#34;&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This defines three outputs, the Function App name, the Cosmos DB uri, and the Cosmos DB key. This is necessary as we didn&amp;rsquo;t specify these values ourselves, so we need AzureRM to &lt;em&gt;tell us what it just created&lt;/em&gt; in order that we can use these names for other tasks in our build process.&lt;/p&gt;

&lt;h3 id=&#34;the-build-definition&#34;&gt;The Build Definition&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/builddefinition.png&#34; alt=&#34;The Build Steps in VSTS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a fairly standard workflow, with a couple of things of note. We build the solution at the beginning and save the artifacts, but before we run our integration test(s?), we run the AzureRM template to &amp;ldquo;Create or Update&amp;rdquo; a resource group, and then deploy our Function App inside this resource group. This is so that we can run our integration tests with the real dependencies. The &amp;ldquo;trick&amp;rdquo;, such as it is, is in the AzureRM deployment step:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/AzureRMStep.png&#34; alt=&#34;AzureRM Deploy step showing parameters&#34; /&gt;.&lt;/p&gt;

&lt;p&gt;By passing the &lt;em&gt;name of the branch we are building&lt;/em&gt; as the name of the resource group, this means that the first time we build any given branch we will get a brand new environment with a Cosmos DB, a Function App, etc. etc., exclusively for that build.&lt;/p&gt;

&lt;p&gt;After the template is deployed, we need a bit of Powershell to retrieve the outputs from the template.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span class=&#34;k&#34;&gt;param&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;no&#34;&gt;[string]&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$resourceGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;nv&#34;&gt;$outputs&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;Get-AzureRmResourceGroupDeployment&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;-ResourceGroupName&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$resourceGroup&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Sort&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Timestamp&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;-Descending&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Select&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;-First&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;

&lt;span class=&#34;nv&#34;&gt;$outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Keys&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;Write-Host&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;##vso[task.setvariable variable=&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;;]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;+&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this does is to find the most recent deployment for the specified resource group - i.e. the one we just did - and map the &lt;em&gt;outputs&lt;/em&gt; of the template to &lt;em&gt;build variables&lt;/em&gt; in VSTS by name. This means that for every output in the template, we need to define a build variable with &lt;em&gt;exactly&lt;/em&gt; the same name, which indeed we have done:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/buildvars.png&#34; alt=&#34;Build variables view&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The values are all empty, as they will be assigned by the above script during each build.&lt;/p&gt;

&lt;p&gt;We can then use these variables in the Function App deployment step to tell it where to deploy our Function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/appServiceDeploy.png&#34; alt=&#34;FunctionAppDeployment&#34; /&gt;&lt;/p&gt;

&lt;p&gt;as well as in the Integration Testing step, where we use these variables to overwrite the values in the &lt;code&gt;runsettings&lt;/code&gt; file used by the &lt;code&gt;vstest&lt;/code&gt; test runner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/runsettings.png&#34; alt=&#34;runsettings&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The final step is just to update the Github mirror of the VSTS repo; in most corporate environments this won&amp;rsquo;t be necessary!&lt;/p&gt;

&lt;h3 id=&#34;the-build-trigger&#34;&gt;The Build Trigger&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/buildtrigger.png&#34; alt=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/buildtrigger.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This build will be run for every new commit on every branch. A moment&amp;rsquo;s reflection will reveal that if the branch is created through the UI on the server, the build will be triggered immediately, as there will be a new branch with a new (to that branch) commit. The first build will always be the slowest, as this is the one that will create the resource group from scratch.&lt;/p&gt;

&lt;p&gt;Equally, a new build will be triggered every time the developer pushes new commits to the feature branch, following which the artifacts will be deployed and the integration tests run, &lt;em&gt;in the private environment dedicated to that branch&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Subsequent builds will be faster; the time penalty associated with creating the resource group from scratch is around two minutes in this particular case, using the hosted queue (2:32 to create from scratch, 0:38 to revalidate on subsequent builds). The overall build time is around 5 minutes the first time, and 3 minutes on subsequent runs. (since there&amp;rsquo;s only one integration test, the slowest part is the nuget restore!).&lt;/p&gt;

&lt;h3 id=&#34;branch-policy&#34;&gt;Branch Policy&lt;/h3&gt;

&lt;p&gt;In addition to enforcing a build on every commit, we can force another build at the point a pull request is made from our feature branch to master:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/masterbranchpolicy.png&#34; alt=&#34;Master branch policy in vsts&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This will &amp;ldquo;pre-merge&amp;rdquo; our feature branch into master, and trigger a new build. This will have the side effect of creating a new resource group called &amp;ldquo;merge&amp;rdquo;, in which the tests will be run again. However, since the same branch name is used for every pull-request build, the &amp;ldquo;merge&amp;rdquo; resource group will only be created once, and won&amp;rsquo;t be modified unless the AzureRM template changes.&lt;/p&gt;

&lt;h3 id=&#34;merging-a-pull-request&#34;&gt;Merging a Pull Request&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/closingapr.png&#34; alt=&#34;Merging a PR&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This screenshot shows the &amp;ldquo;moment of truth&amp;rdquo; immediately prior to merging a pull request. The &amp;ldquo;pre-merged&amp;rdquo; branch has been built successfully, and when we click &amp;ldquo;Complete Merge&amp;rdquo; the source branch will be deleted. The resource group, however, will still be hanging around incurring charges, and this could become an expensive business if we are creating a new environment for every single feature and never tearing them down.&lt;/p&gt;

&lt;h3 id=&#34;tearing-down-the-environment&#34;&gt;Tearing down the environment&lt;/h3&gt;

&lt;p&gt;This is achieved by calling an Azure Automation runbook in a Service Hook from VSTS.&lt;/p&gt;

&lt;p&gt;To create the hook, we first need to define our Automation runbook and give it a web endpoint. The integration of Azure Automation and source control is a &amp;ldquo;work in progress&amp;rdquo;, so I&amp;rsquo;ve stored the body of the runbook in a gist&lt;/p&gt;

&lt;p&gt;&lt;script src=&#34;//gist.github.com/gavincampbell/a12a87477bd38402b310262040509a56.js&#34;&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This script will parse the JSON payload from VSTS, find the source branch name (from the refspec), and delete the resource group with that name. This runbook is associated with an endpoint url, which we provide to VSTS in the Web hook configuration.&lt;/p&gt;

&lt;p&gt;The runbook requires an Azure Automation Account.  I have created these in a separate resource group, not &amp;ldquo;managed&amp;rdquo; from VSTS.&lt;/p&gt;

&lt;p&gt;We set the event to track to &amp;ldquo;Pull request merge commit created&amp;rdquo; - there&amp;rsquo;s no &amp;ldquo;branch deleted&amp;rdquo; event available here. Remember, the branch gets deleted when the PR is merged, assuming the box is ticked. Notably, we are only firing this event on merges &lt;em&gt;to&lt;/em&gt; master.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/servicehookp1.png&#34; alt=&#34;Service Hook p1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On the second page of the dialog, we just supply the endpoint url for our Automation runbook.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/servicehookp2.png&#34; alt=&#34;Service Hook p2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;release-management&#34;&gt;Release Management&lt;/h2&gt;

&lt;p&gt;Once the branch is merged to master and its environment torn down, then what? We release the feature, of course:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/releaseprocess.png&#34; alt=&#34;Release Management in VSTS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This Release definition is triggered on every new build of the master branch. Since the branch is protected by a policy, these will only occur when a pull request has been created, successfully built, reviewed, and merged.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/releasetrigger.png&#34; alt=&#34;Release Trigger&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As soon as the release is triggered, we do a deployment to QA. The &amp;ldquo;little man with a clock&amp;rdquo; next to the &amp;ldquo;Production&amp;rdquo; environment indicated that there is a manual approval required before a release can be deployed to production.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/qatrigger.png&#34; alt=&#34;QA trigger&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-deployment-process&#34;&gt;The deployment process&lt;/h3&gt;

&lt;p&gt;As is typical, the Release Definition is much simpler than the Build Definition. In this instance it consists of only two steps, deploying the Azure RM template and deploying the Function to our Function App.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/deploymenttasks.png&#34; alt=&#34;Release tasks in VSTS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s notable though, is that we are using the same Azure RM template for QA and Production as we used for the ephemeral development environments. The only difference is that instead of allowing the parameters to default to their &amp;ldquo;unique string&amp;rdquo; values, we explicitly set all these parameters to &amp;ldquo;well known&amp;rdquo; values in the step definition:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/overridetemplateparameters.png&#34; alt=&#34;Azure RM Release Step Definition&#34; /&gt;&lt;/p&gt;

&lt;p&gt;as well as in the step that deploys the function to our function app:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/overrideAppServicename.png&#34; alt=&#34;Azure Function Release Step Definition&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Production environment definition is exactly the same as the QA one, except that the &amp;ldquo;well known&amp;rdquo; values for the resource names are different.&lt;/p&gt;

&lt;p&gt;This means that it doesn&amp;rsquo;t matter if the QA and Prod environments are maintained by a different team, or if they are created in a different subscription or in a different tenant; as long as we know the values to plug into our template, we can override our defaults with the &amp;ldquo;permanent&amp;rdquo; names.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;permanent&amp;rdquo; resource names still need to respect the rules for uniqueness though, so if these names are important it is probably wise to create these resources in advance to avoid disappointment at deployment time.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;re still reading, you&amp;rsquo;ll be aware that this has been a somewhat &amp;ldquo;epic&amp;rdquo; article. The scenario outlined above allows us to automatically create and tear down a new environment for every single feature. Under normal circumstances, there will be four &amp;ldquo;persistent&amp;rdquo; resource groups, namely &amp;ldquo;QA&amp;rdquo; and &amp;ldquo;Production&amp;rdquo;, as well as &amp;ldquo;merge&amp;rdquo; - used for building and testing &amp;ldquo;pre-merged&amp;rdquo; pull requests, and &amp;ldquo;master&amp;rdquo; - used to rebuild and retest &amp;ldquo;master&amp;rdquo; after every merge commit.&lt;/p&gt;

&lt;p&gt;The extra resource group for Azure Automation is also visible in this screenshot, as is the resource group for a feature branch, which will be automatically torn down when the branch is merged.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/vsts-azurerm/ResourceGroups.png&#34; alt=&#34;resource groups in the Azure Portal&#34; /&gt;&lt;/p&gt;

&lt;p&gt;All of the above is still a work in progress, if there are any glaring errors or omissions please do get in touch via the comments or via the &lt;a href=&#34;http://arapaima.uk/fixed/about/&#34;&gt;contact form&lt;/a&gt; on the site.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Using Vagrant to create a Virtual Machine running SQL Server on CentOS Linux</title>
          <link>http://arapaima.uk/post/2017-06-17-vagrant-sql-server-linux-libvirt/</link>
          <pubDate>Sat, 17 Jun 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-06-17-vagrant-sql-server-linux-libvirt/</guid>
          <description>

&lt;p&gt;In my mind, the ability to do this kind of thing is the really big &amp;ldquo;win&amp;rdquo; with SQL Server on Linux. In their own words,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt; is a tool for building and managing virtual machine environments in a single workflow. With an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases production parity, and makes the &amp;ldquo;works on my machine&amp;rdquo; excuse a relic of the past.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;rsquo;ve been doing &lt;a href=&#34;http://arapaima.uk/fixed/speaking/&#34;&gt;presentations&lt;/a&gt; at conferences and meetups for a few years, so I&amp;rsquo;m familiar with the pain of setting up &amp;ldquo;Demo VMs&amp;rdquo; for SQL Server on Windows. I also have less than fond memories of setting up and maintaining multiple VMs on a single host to reproduce client problems with clustering, replication, etc.; fortunately I don&amp;rsquo;t tend to get involved with this kind of thing too much these days, so it&amp;rsquo;s not in the scope of this example.&lt;/p&gt;

&lt;p&gt;Anyway, with the advent of SQL Server on Linux, it struck me that &lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt;, which I&amp;rsquo;d used for some other stuff in the &amp;ldquo;day job&amp;rdquo; might be a way out of this spiral of despair.&lt;/p&gt;

&lt;p&gt;The promise is that having set up a couple of config files, we can stand up a new VM with everything configured and in the right place, merely by typing &lt;code&gt;vagrant up&lt;/code&gt; at a command prompt, and remove it again by typing &lt;code&gt;vagrant destroy&lt;/code&gt;. If we want to keep it around for another day, &lt;code&gt;vagrant halt&lt;/code&gt; is at our disposal. We can even &lt;code&gt;ssh&lt;/code&gt; to our new VM by typing &lt;code&gt;vagrant ssh&lt;/code&gt;, without any fiddling around with keys or passwords.&lt;/p&gt;

&lt;h2 id=&#34;preamble&#34;&gt;Preamble&lt;/h2&gt;

&lt;p&gt;Yes, I&amp;rsquo;m aware it&amp;rsquo;s possible to manage Windows guests with Vagrant, even from a Windows host, but my experience of trying this is that it&amp;rsquo;s a world of pain, not least because the Windows box files are 800lb gorillas. The most complete attempt I&amp;rsquo;ve seen has been by &lt;a href=&#34;https://chocolatey.org/&#34;&gt;chocolately&lt;/a&gt; creator &lt;a href=&#34;https://github.com/ferventcoder/vagrant-windows-puppet&#34;&gt;Rob Reynolds&lt;/a&gt;, who I suppose has (or had) some fairly unique needs in this area.&lt;/p&gt;

&lt;p&gt;My own use case is further complicated by the fact that I&amp;rsquo;ve been using &lt;a href=&#34;https://getfedora.org/&#34;&gt;Fedora Linux&lt;/a&gt; on the desktop for the last few years, which doesn&amp;rsquo;t &lt;em&gt;really&lt;/em&gt; support Virtualbox - though I&amp;rsquo;m aware it does &lt;em&gt;mostly&lt;/em&gt; work - but prefers &lt;a href=&#34;https://www.linux-kvm.org&#34;&gt;kvm&lt;/a&gt;-based virtual machines which can be managed through a number of utilities, including the supposedly idiot-proof &lt;a href=&#34;https://wiki.gnome.org/Apps/Boxes&#34;&gt;Gnome Boxes&lt;/a&gt;. There are some slightly outdated results from &lt;a href=&#34;http://www.phoronix.com/scan.php?page=article&amp;amp;item=ubuntu-1510-virt&#34;&gt;Phoronix&lt;/a&gt; that suggest there may be some performance gains from using KVM as opposed to Virtualbox.&lt;/p&gt;

&lt;h2 id=&#34;anyway-on-with-the-show&#34;&gt;Anyway, on with the show&amp;hellip;&lt;/h2&gt;

&lt;p&gt;There are three supported platforms (plus Docker) listed on the &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup&#34;&gt;installation guide&lt;/a&gt; for SQL Server on Linux, namely Ubuntu, Suse, and RedHat.&lt;/p&gt;

&lt;p&gt;Given that, I decided to proceed with the creation of a &lt;a href=&#34;https://www.centos.org/&#34;&gt;Centos&lt;/a&gt; VM, given that it&amp;rsquo;s &amp;ldquo;quite like&amp;rdquo; RedHat, and also has a vendor-created image in the &lt;a href=&#34;https://atlas.hashicorp.com/centos/boxes/7&#34;&gt;Vagrant Catalog&lt;/a&gt; that supports libvirt (almost all boxes, including the Ubuntu ones, are VirtualBox only). Suse also has vendor images for libvirt, but I haven&amp;rsquo;t used Suse Linux since the Spice Girls were topping the charts and wasn&amp;rsquo;t inclined to investigate further.&lt;/p&gt;

&lt;h3 id=&#34;the-vagrantfile&#34;&gt;The Vagrantfile&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://www.vagrantup.com/docs/vagrantfile/&#34;&gt;Vagrantfile&lt;/a&gt; is a file called &lt;code&gt;Vagrantfile&lt;/code&gt; that is used to tell Vagrant what the VM you want to create needs to look like.&lt;/p&gt;

&lt;p&gt;In this case it&amp;rsquo;s fairly simple, but I&amp;rsquo;ve left the autogenerated comments in the version in the &lt;a href=&#34;https://github.com/gavincampbell/VagrantSqlServerCentosEtc&#34;&gt;github repo that accompanies this post&lt;/a&gt; (they&amp;rsquo;re useful if, as is almost certain, you want to reproduce this on a slighly different platform to me, i.e. Virtualbox!), so I&amp;rsquo;ve extracted the highlights to a &lt;a href=&#34;https://gist.github.com/gavincampbell/a9b920ff7b1c7f3547aeeca46e186050&#34;&gt;gist&lt;/a&gt; here.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/gavincampbell/a9b920ff7b1c7f3547aeeca46e186050.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We specify the &amp;ldquo;base box&amp;rdquo;, in this case Centos 7 on which our VM is going to be based. After that, we just specify the ways in which our VM is going to diverge from this base. In our case, we are setting up a forwarded port for 1433, so that clients on the host can connect to the SQL Server as if it were &amp;ldquo;local&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re increasing the default memory allocation to the VM from the default (which I think is 512MB) to 4GB - the minimum requirement for SQL Server on Linux is 3.25, and I have read that the installer will fail if you have less than this available.&lt;/p&gt;

&lt;p&gt;After that, we specify a shell script to run to set up everything else; In Vagrant-speak this is known as a &amp;ldquo;provisioner&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Vagrant doesn&amp;rsquo;t have to use shell scripts for configuration, it supports a number of alternative provisioners, such as the usual suspects of &lt;a href=&#34;https://www.ansible.com/&#34;&gt;Ansible&lt;/a&gt;, &lt;a href=&#34;https://www.chef.io/chef/&#34;&gt;Chef&lt;/a&gt;, and &lt;a href=&#34;https://puppet.com/&#34;&gt;Puppet&lt;/a&gt;, in addition to a couple of others.&lt;/p&gt;

&lt;p&gt;The advantage of these latter approaches, of course, is that they are idempotent, making the scripts easy to build up over time; I intend to revisit this example, probably with Ansible, once SQL Server on Linux is generally available and the installation procedure is a bit less &amp;ldquo;dynamic&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The shell script fetches updates with &lt;code&gt;yum&lt;/code&gt; (for those more accustomed to Debian-derivatives such as Ubuntu this is like &lt;code&gt;apt&lt;/code&gt;), then adds the Microsoft repository definitions to &lt;code&gt;yum&lt;/code&gt;&amp;rsquo;s configuration. It also installs a package called &lt;code&gt;tcping&lt;/code&gt; from another repo called &lt;code&gt;epel&lt;/code&gt; (which stands for Extra Packages for Enterprise Linux), which we&amp;rsquo;re going to need in our script. Having got everything downloaded, we install the SQL Server client and server, which are separate packages.&lt;/p&gt;

&lt;p&gt;Not only are they separate packages, they require different mechanisms for accepting the terms of the EULA; one requires an environment variable &lt;code&gt;ACCEPT_EULA=y&lt;/code&gt;, and the other requires a parameter &lt;code&gt;accept-eula&lt;/code&gt;!&lt;/p&gt;

&lt;p&gt;We also pass the top-secret &lt;code&gt;sa&lt;/code&gt; password as an environment variable; this is required to run the installation silently.&lt;/p&gt;

&lt;p&gt;Having done all that, we wait for the service to start before proceeding. This is why &lt;code&gt;tcping&lt;/code&gt; was required, it&amp;rsquo;s among the simplest ways to figure out if there&amp;rsquo;s anything listening on a given port.&lt;/p&gt;

&lt;p&gt;Finally, we restore a database from a backup and run a script to install the tSQLt unit testing framework. By default, Vagrant will rsync the folder containing the &lt;code&gt;Vagrantfile&lt;/code&gt; to a &lt;code&gt;/Vagrant&lt;/code&gt; folder inside our VM, so we can simply put any files we need for provisioning (or for anything else) inside this folder. In this case, I&amp;rsquo;ve extracted the contents of the &lt;code&gt;tSQLt.zip&lt;/code&gt; from the &lt;a href=&#34;http://tsqlt.org/downloads/&#34;&gt;tSQLt downloads page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Originally, I had the database backup copied from the &lt;a href=&#34;https://github.com/microsoft/sql-server-samples&#34;&gt;original repo&lt;/a&gt; here too, but had to replace this with a call to &lt;code&gt;curl&lt;/code&gt;; I&amp;rsquo;m glad to say I&amp;rsquo;d never run into the Github file size limit before! Obviously this makes the re-provisioning process a bit slower, it&amp;rsquo;s best to have these large file dependencies somewhere local if you can manage it.&lt;/p&gt;

&lt;h3 id=&#34;what-we-didn-t-do&#34;&gt;What we didn&amp;rsquo;t do&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s nothing in the (abbreviated) &lt;code&gt;Vagrantfile&lt;/code&gt; about networking (other than the forwarded port), storage, logins, cores, etc. etc. All these things are configurable, but the point of this approach is that we trust Vagrant to &amp;ldquo;do the right thing&amp;rdquo; unless we specify otherwise.&lt;/p&gt;

&lt;h2 id=&#34;the-moment-of-truth&#34;&gt;The Moment of Truth&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;vagrant up&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;profit&#34;&gt;Profit&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/sql-linux-vagrant/vscode.png&#34; alt=&#34;vscode on fedora connected to linux on centos in vagrant!&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To be clear, this is a picture of Visual Studio Code, running on Fedora Linux 25 (the host), connected to SQL Server running on Centos 7.3 (the guest) in a virtual machine provisioned by vagrant. The mssql extension for Visual Studio Code is a very recent alpha; this was to do with .net core compatibility on &amp;ldquo;modern&amp;rdquo; versions of Linux.&lt;/p&gt;

&lt;p&gt;The VM is a &amp;ldquo;regular&amp;rdquo; VM, you can see it here in the Fedora Virtual Machine Manager application with the console open showing the SQL Server process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/sql-linux-vagrant/vmm.png&#34; alt=&#34;vagrant vm opened in virt-manager showing the sql server process&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-wrap&#34;&gt;The Wrap&lt;/h2&gt;

&lt;p&gt;I was slightly surprised that all this worked as well as it did; not so much the Vagrant part as the SQL Server on Linux part, which is certainly more complete than when I last looked at it. In particular, I wasn&amp;rsquo;t expecting to be able to install tSQLt on Linux - I did have to make a change to the &lt;code&gt;SetClrEnabled.sql&lt;/code&gt; script that is distributed with tSQLt to turn off &lt;code&gt;clr strict security&lt;/code&gt;, but apart from that it all went pretty smoothly. I have a &lt;a href=&#34;http://www.sqlsaturday.com/645/Sessions/Details.aspx?sid=63722&#34;&gt;presentation about tSQLt&lt;/a&gt; to do next month, which was one of the motivations for this exercise, and I&amp;rsquo;ll certainly be setting aside some time in the next day or two to see if everything else works the way one might expect. If you want to try this out at home, and assuming your setup is roughly like mine (kvm rather than Virtualbox, vagrant already working, etc, etc):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir hereGoesNothing &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; hereGoesNothing&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
git clone https://github.com/gavincampbell/VagrantSqlServerCentosEtc .
vagrant up&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If your setup is different in relevant ways, there will be some steps between 2 and 3 where you install things and hack away at the &lt;code&gt;Vagrantfile&lt;/code&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Enabling per-branch configuration in a Jenkins Multibranch Pipeline</title>
          <link>http://arapaima.uk/post/2017-05-25-conditional-processing-jenkins-multibranch/</link>
          <pubDate>Thu, 25 May 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-05-25-conditional-processing-jenkins-multibranch/</guid>
          <description>

&lt;p&gt;For reasons, you might want your &lt;a href=&#34;https://jenkins.io/doc/book/pipeline/multibranch/&#34;&gt;Jenkins Multibranch Pipeline&lt;/a&gt; jobs to do a different thing depending on which branch is being built.&lt;/p&gt;

&lt;p&gt;Fortunately, the multibranch plugin provides us with a built-in variable &lt;code&gt;BRANCH_NAME&lt;/code&gt;, which we can use to figure out which branch we are currently building.&lt;/p&gt;

&lt;p&gt;In such scenarios, it&amp;rsquo;s not a bad idea to create a minimal &lt;code&gt;Jenkinsfile&lt;/code&gt; at the repo root that contains just enough logic to figure out which branch we are on,  and then call another groovy script that contains the actual build definition:&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1.js?file=Jenkinsfile&#34;&gt;&lt;/script&gt;

&lt;p&gt;In this script we load an external groovy file based on the current branch, and then call the function defined therein. In this particular case, the &lt;code&gt;run_build()&lt;/code&gt; functions don&amp;rsquo;t do anything particularly exciting, but they probably do enough to demonstrate this mechanism.&lt;/p&gt;

&lt;h4 id=&#34;the-script-for-the-master-branch&#34;&gt;The script for the master branch:&lt;/h4&gt;

&lt;script src=&#34;//gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1.js?file=master.groovy&#34;&gt;&lt;/script&gt;

&lt;h4 id=&#34;the-script-for-any-other-branch&#34;&gt;The script for any other branch:&lt;/h4&gt;

&lt;script src=&#34;//gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1.js?file=not-master.groovy&#34;&gt;&lt;/script&gt;

&lt;p&gt;The most important line in each of these scripts is the &lt;code&gt;return this&lt;/code&gt; at the end; as &lt;a href=&#34;https://jenkins.io/doc/pipeline/steps/workflow-cps/#code-load-code-evaluate-a-groovy-source-file-into-the-pipeline-script&#34;&gt;documented&lt;/a&gt;, this is required for the functions to be callable from the outer script. The &lt;code&gt;checkout scm&lt;/code&gt; step in the root &lt;code&gt;Jenkinsfile&lt;/code&gt; is also required, as without it the rest of the scripts won&amp;rsquo;t be fetched from the repo. All three of these files are in the root of the repo, this is because gist doesn&amp;rsquo;t support folders. In &amp;ldquo;real life&amp;rdquo;, it&amp;rsquo;s probably a good idea to create a separate folder for these scripts, and provide the path to the &lt;code&gt;load&lt;/code&gt; function.&lt;/p&gt;

&lt;h2 id=&#34;trying-this-out-at-home&#34;&gt;Trying this out at home&lt;/h2&gt;

&lt;p&gt;If you don&amp;rsquo;t have a Jenkins server handy, you can create a new one with &lt;code&gt;docker run -p 8080:8080 jenkinsci/jenkins:lts&lt;/code&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and connect to it on &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;. After that, it&amp;rsquo;s just a question of creating a new job of type &amp;ldquo;Mutibranch Pipeline&amp;rdquo; and specifying an appropriate name.&lt;/p&gt;

&lt;p&gt;In the &amp;ldquo;Branch Sources&amp;rdquo; section, add a source of type &amp;ldquo;Git&amp;rdquo; (not &amp;ldquo;Github&amp;rdquo;!), and provide it with the path to the source repo, in this case the gist.&lt;/p&gt;

&lt;h3 id=&#34;using-a-gist-as-a-source-repo&#34;&gt;Using a gist as a source repo?&lt;/h3&gt;

&lt;p&gt;It might be a lesser known fact about Github that gists are &lt;a href=&#34;https://help.github.com/articles/forking-and-cloning-gists/&#34;&gt;really just repos with a few extra rules&lt;/a&gt; that can be forked and cloned like any other Github repo. What it doesn&amp;rsquo;t say in that linked page is that you can also create branches, even if these new branches aren&amp;rsquo;t visible in the Gist UI. So, if you create a fork of &lt;a href=&#34;https://gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1&#34;&gt;https://gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1&lt;/a&gt;, you should end up with an identical gist in your own Github account. You can then paste the url of this gist into the Jenkins job definition as shown:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-multibranch-gist/multibranch-pipeline-from-gist.png&#34; alt=&#34;Gist url in git scm step&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The other thing to note here is the checkbox for &amp;ldquo;Scan Multibranch Pipeline Triggers&amp;rdquo;; since we aren&amp;rsquo;t configuring any push notifications from our git repo, we need to get Jenkins to scan the repo periodically to look for any new branches, or new commits in existing branches. (In my particular case, since the Jenkins instance is just an ephemeral Docker image, there&amp;rsquo;s no route to it anyway.)&lt;/p&gt;

&lt;p&gt;When you save the job configuration, Jenkins will scan the source repo, and create the first pipeline job:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-multibranch-gist/master-pipeline-job.png&#34; alt=&#34;pipeline job for master branch&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;creating-a-new-branch&#34;&gt;Creating a new branch&lt;/h3&gt;

&lt;p&gt;As noted above, there&amp;rsquo;s nothing in the gist UI to support creating new branches. However, if we clone the gist repo to a local folder we can create the branch and push it back to the origin. You can substitute the url to your own fork of the gist in place of mine (there&amp;rsquo;s only one branch in mine, and you don&amp;rsquo;t have permission to create more!). The downside of this approach of using a gist rather than a &amp;ldquo;proper&amp;rdquo; repo is that by default you get a long guid instead of a readable folder name.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ mkdir j &amp;amp;&amp;amp; cd j;
$ git clone https://gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1
Cloning into &amp;#39;480843552e43efa84c60f9bb4840d6c1&amp;#39;...
remote: Counting objects: 32, done.
remote: Compressing objects: 100% (32/32), done.
remote: Total 32 (delta 7), reused 0 (delta 0), pack-reused 0
Unpacking objects: 100% (32/32), done.
Checking connectivity... done.
$ cd 480843552e43efa84c60f9bb4840d6c1/
$ git status
On branch master
Your branch is up-to-date with &amp;#39;origin/master&amp;#39;.
nothing to commit, working tree clean
$ git checkout -b fancyFeature
Switched to a new branch &amp;#39;fancyFeature&amp;#39;
$ git push --set-upstream origin fancyFeature

Total 0 (delta 0), reused 0 (delta 0)
To https://gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1
 * [new branch]      fancyFeature -&amp;gt; fancyFeature
Branch fancyFeature set up to track remote branch fancyFeature from origin.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The next time the repo is scanned,  the new job will be created in Jenkins:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-multibranch-gist/job-created-for-new-branch.png&#34; alt=&#34;job created for new branch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When the branch is deleted, the job wil disappear too:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ git push origin --delete fancyFeature

To https://gist.github.com/gavincampbell/480843552e43efa84c60f9bb4840d6c1
 - [deleted]         fancyFeature&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looking in the &amp;ldquo;Multibranch Pipeline Log&amp;rdquo; confirms this:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;[Thu May 25 22:40:36 UTC 2017] Finished branch indexing. Indexing took 1.7 sec
Evaluating orphaned items in jenkins-branch-conditions
Will remove fancyFeature as it is #1 in the list
Finished: SUCCESS&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Depending on your system configuration, this step may have one or more prerequisites!
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Automating SSDT build and deployment with Jenkins from a local git repo</title>
          <link>http://arapaima.uk/post/2017-04-04-jenkins-windows-git-ssdt-profit/</link>
          <pubDate>Tue, 04 Apr 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-04-04-jenkins-windows-git-ssdt-profit/</guid>
          <description>

&lt;p&gt;This is a short illustration of using a local installation of Jenkins on Windows to build an SSDT project from a local git repo and deploy it to a SQL Server on the same machine.&lt;/p&gt;

&lt;p&gt;This is probably useful for a quick demonstration or to understand how the various moving parts fit together, but possibly less applicable to &amp;ldquo;Real Life&amp;rdquo; production environments.&lt;/p&gt;

&lt;p&gt;There are no build agents and no git remotes; all the action takes place on the Jenkins master, and the git repo is local to the same machine. No &amp;ldquo;git management&amp;rdquo; software such as GitHub/VSTS/GitLab/BitBucket/etc is involved (except for some slight cheating regarding the build definition itself.)&lt;/p&gt;

&lt;h2 id=&#34;getting-set-up&#34;&gt;Getting set up&lt;/h2&gt;

&lt;p&gt;I am using Windows 10 with SQL Server 2016 Developer Edition and Visual Studio 2015 Professional. Most of this stuff should work with most editions and versions of SQL Server and Visual Studio, but some of the paths will vary here and there. I am using SQL Server Data Tools Version 14.0.61021.0, released on October 26, 2016.&lt;/p&gt;

&lt;h3 id=&#34;installing-jenkins&#34;&gt;Installing Jenkins&lt;/h3&gt;

&lt;p&gt;I used the windows installer linked &lt;a href=&#34;https://jenkins.io/content/thank-you-downloading-windows-installer/#stable&#34;&gt;here&lt;/a&gt; (note that this link will trigger the download &lt;em&gt;immediately&lt;/em&gt;). At the time of writing this is version 2.46.1. In the initial setup, I selected the option to &amp;ldquo;Install Recommended Plugins&amp;rdquo;; this installs more than is required for this example.&lt;/p&gt;

&lt;h3 id=&#34;further-jenkins-twiddling&#34;&gt;Further Jenkins twiddling&lt;/h3&gt;

&lt;p&gt;The MSBuild plugin needs to be installed separately (as it isn&amp;rsquo;t &amp;ldquo;&lt;em&gt;recommended&lt;/em&gt;&amp;rdquo;!), which can be done from &lt;a href=&#34;http://localhost:8080/pluginManager/available&#34;&gt;http://localhost:8080/pluginManager/available&lt;/a&gt; or by clicking through &lt;em&gt;Jenkins -&amp;gt; Manage Jenkins -&amp;gt; Manage Plugins -&amp;gt; Available&lt;/em&gt;. We need to tell Jenkins where it can find &lt;code&gt;MSBuild.exe&lt;/code&gt;, this is done on &lt;a href=&#34;http://localhost:8080/configureTools/&#34;&gt;http://localhost:8080/configureTools/&lt;/a&gt; (&lt;em&gt;Jenkins -&amp;gt; Manage Jenkins -&amp;gt; Global Tool Configuration&lt;/em&gt;). You can find out where &lt;code&gt;MSBuild&lt;/code&gt; is installled by opening the &amp;ldquo;Developer Command Prompt for Visual Studio&amp;rdquo; and typing&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Program&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt; Files (x86)\Microsoft Visual Studio 14.0&amp;gt;where msbuild&lt;/span&gt;
C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Program&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt; Files (x86)\MSBuild\14.0\Bin\MSBuild.exe&lt;/span&gt;
C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Windows\Microsoft.NET\Framework\v4.0.30319\MSBuild.exe&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I used the first of these, as this is the one installed by Visual Studio&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; 2015, and should avoid having to re-read the somewhat convoluted StackOverflow thread &lt;a href=&#34;http://stackoverflow.com/questions/22968561/msbuild-errors-for-database-project-on-tfs-server-with-vs-2013-shell&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I already had git installed and in the path, so there was no need to configure this separately in Jenkins.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ git --version
git version &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;.11.0.windows.3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;compromising-your-sql-server-2&#34;&gt;Compromising your SQL Server &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;p&gt;By default, the Jenkins service will run as &amp;ldquo;Local System&amp;rdquo; on Windows. In order to allow this account - which will authenticate to SQL Server as the machine account - to deploy the database, I made &amp;ldquo;Local System&amp;rdquo; an &lt;code&gt;sa&lt;/code&gt; of the SQL Server:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;k&#34;&gt;EXEC&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sp_addsrvrolemember&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;NT AUTHORITY\SYSTEM&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sysadmin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;There are many circumstances in which this wouldn&amp;rsquo;t be a good idea, if any of them apply in your scenario you should make appropriate adjustments to the scripts that follow.&lt;/p&gt;

&lt;h3 id=&#34;find-something-to-build&#34;&gt;Find something to build&lt;/h3&gt;

&lt;p&gt;I started by cloning a repo I had prepared earlier, containing a copy of the Chinook database.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Projects&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&amp;gt;git clone https://github.com/arapaima-uk/Chinook.JenkinsDemo.git&lt;/span&gt;
Cloning into &amp;#39;Chinook.JenkinsDemo&amp;#39;...
remote: Counting objects: 26, done.
remote: Compressing objects: 100% (20/20), done.
remote: Total 26 (delta 5), reused 26 (delta 5), pack-reused 0
Unpacking objects: 100% (26/26), done.

C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Projects&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&amp;gt;tree&lt;/span&gt;
Folder PATH listing
Volume serial number is 000000B7 806E&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;E890&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;
C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;
└───Chinook.JenkinsDemo
    └───Chinook.JenkinsDemo
        └───dbo
            └───Tables

C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Projects&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;having done that, I then removed the reference to the GitHub remote with&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Projects\Chinook.JenkinsDemo&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&amp;gt;git remote rm origin&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;From here on, everything is local (&lt;em&gt;geddit?&lt;/em&gt;).&lt;/p&gt;

&lt;h2 id=&#34;creating-the-jenkins-job&#34;&gt;Creating the Jenkins job&lt;/h2&gt;

&lt;p&gt;The first step is to create the Jenkins job that will build our project into a dacpac, and deploy it to a local SQL Server.&lt;/p&gt;

&lt;p&gt;The job definition is contained in the following Jenkinsfile, which consists of three fairly self-explanatory stages. The first stage &lt;code&gt;git checkout&lt;/code&gt; checks out our master branch from the local (indicated by the &lt;code&gt;file:\\&lt;/code&gt; prefix) repo. The second stage calls the &lt;code&gt;MSBuild&lt;/code&gt; tool we defined earlier, taking advantage of the fact that our project is very simple to provide very few parameters on the command line. The third calls out to the shell to call &lt;code&gt;sqlpackage.exe&lt;/code&gt;, again with very few parameters. This last stage is the one that requires whatever account Jenkins is running under to be able to authenticate to the SQL Server (though there are alternatives involving storing credentials in Jenkins).&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/gavincampbell/e3dfa25abf427752e919eb2110f03852.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Conveniently, since this is stored in a GitHub gist, and a gist is a git repo, I can create a new pipeline job and specify &amp;ldquo;Pipeline Script from SCM&amp;rdquo; in the build definition then provide the url of the gist for the repository url.&lt;/p&gt;

&lt;p&gt;Note that this repo contains a single file, namely the build definition - all the project files are coming from the git repo on our local machine. If you want to copy and paste the complete url, it&amp;rsquo;s in the build output log reproduced below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-ssdt-git/pipeline-job-from-gist.PNG&#34; alt=&#34;Screenshot of Jenkins showing pipeline from SCM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One more detail is that the job needs to be set to &amp;ldquo;Poll SCM&amp;rdquo;, but the schedule can be left empty (ignore the warning). This isn&amp;rsquo;t required to test the build, but will be required later on to trigger the build on a commit to our git repo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-ssdt-git/jenkins-job-set-to-poll-with-empty-schedule.PNG&#34; alt=&#34;Jenkins Pipeline Job showing Poll SCM Setting with empty schedule&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;testing-the-build&#34;&gt;Testing the build&lt;/h3&gt;

&lt;p&gt;We should now be able to trigger the build from the Jenkins dashboard, and see some output like the following (under &amp;ldquo;Console Output&amp;rdquo;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Started by user arapaima
Obtained Jenkinsfile from git https://gist.github.com/gavincampbell/e3dfa25abf427752e919eb2110f03852
[Pipeline] node
Running on master in C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo
[Pipeline] {
[Pipeline] stage
[Pipeline] { (git checkout)
[Pipeline] git
 &amp;gt; git.exe rev-parse --is-inside-work-tree # timeout=10
Fetching changes from the remote Git repository
 &amp;gt; git.exe config remote.origin.url file:///C:/Projects/Chinook.JenkinsDemo # timeout=10
Fetching upstream changes from file:///C:/Projects/Chinook.JenkinsDemo
 &amp;gt; git.exe --version # timeout=10
 &amp;gt; git.exe fetch --tags --progress file:///C:/Projects/Chinook.JenkinsDemo +refs/heads/*:refs/remotes/origin/*
 &amp;gt; git.exe rev-parse &amp;quot;refs/remotes/origin/master^{commit}&amp;quot; # timeout=10
 &amp;gt; git.exe rev-parse &amp;quot;refs/remotes/origin/origin/master^{commit}&amp;quot; # timeout=10
Checking out Revision 8ccbac95d2edd4ce0cbf14ec9f5f3f7ac2868eac (refs/remotes/origin/master)
 &amp;gt; git.exe config core.sparsecheckout # timeout=10
 &amp;gt; git.exe checkout -f 8ccbac95d2edd4ce0cbf14ec9f5f3f7ac2868eac
 &amp;gt; git.exe branch -a -v --no-abbrev # timeout=10
 &amp;gt; git.exe branch -D master # timeout=10
 &amp;gt; git.exe checkout -b master 8ccbac95d2edd4ce0cbf14ec9f5f3f7ac2868eac
 &amp;gt; git.exe rev-list 8ccbac95d2edd4ce0cbf14ec9f5f3f7ac2868eac # timeout=10
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Build Dacpac from SQLProj)
[Pipeline] tool
[Pipeline] bat
[BuildDeploySsdtFromLocalRepo] Running batch script

C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo&amp;gt;&amp;quot;C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe&amp;quot;  /p:Configuration=Release 
Microsoft (R) Build Engine version 14.0.25420.1
Copyright (C) Microsoft Corporation. All rights reserved.

Building the projects in this solution one at a time. To enable parallel build, please add the &amp;quot;/m&amp;quot; switch.
Build started 05/04/2017 00:55:50.
Project &amp;quot;C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo.sln&amp;quot; on node 1 (default targets).
ValidateSolutionConfiguration:
  Building solution configuration &amp;quot;Release|Any CPU&amp;quot;.
Project &amp;quot;C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo.sln&amp;quot; (1) is building &amp;quot;C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo\Chinook.JenkinsDemo.sqlproj&amp;quot; (2) on node 1 (default targets).
GenerateSqlTargetFrameworkMoniker:
Skipping target &amp;quot;GenerateSqlTargetFrameworkMoniker&amp;quot; because all output files are up-to-date with respect to the input files.
CoreCompile:
Skipping target &amp;quot;CoreCompile&amp;quot; because all output files are up-to-date with respect to the input files.
SqlBuild:
Skipping target &amp;quot;SqlBuild&amp;quot; because all output files are up-to-date with respect to the input files.
CopyFilesToOutputDirectory:
  Chinook.JenkinsDemo -&amp;gt; C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo\bin\Release\Chinook.JenkinsDemo.dll
SqlPrepareForRun:
  Chinook.JenkinsDemo -&amp;gt; C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo\bin\Release\Chinook.JenkinsDemo.dacpac
Done Building Project &amp;quot;C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo\Chinook.JenkinsDemo.sqlproj&amp;quot; (default targets).
Done Building Project &amp;quot;C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo\Chinook.JenkinsDemo.sln&amp;quot; (default targets).

Build succeeded.
    0 Warning(s)
    0 Error(s)

Time Elapsed 00:00:00.53
[Pipeline] stash
Stashed 1 file(s)
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Deploy Dacpac to SQL Server)
[Pipeline] unstash
[Pipeline] bat
[BuildDeploySsdtFromLocalRepo] Running batch script

C:\Program Files (x86)\Jenkins\workspace\BuildDeploySsdtFromLocalRepo&amp;gt;&amp;quot;C:\Program Files (x86)\Microsoft SQL Server\130\DAC\bin\sqlpackage.exe&amp;quot; /Action:Publish /SourceFile:&amp;quot;Chinook.JenkinsDemo\bin\Release\Chinook.JenkinsDemo.dacpac&amp;quot; /TargetServerName:(local) /TargetDatabaseName:Chinook 
Publishing to database &#39;Chinook&#39; on server &#39;(local)&#39;.
Initializing deployment (Start)
Initializing deployment (Complete)
Analyzing deployment plan (Start)
Analyzing deployment plan (Complete)
Updating database (Start)
Update complete.
Updating database (Complete)
Successfully published database.
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
Finished: SUCCESS

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &amp;ldquo;stage view&amp;rdquo; of the build should be showing green:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-ssdt-git/passing-stage-view.PNG&#34; alt=&#34;passing builds in Jenkins&#34; title=&#34;Earlier builds omitted for clarity, naturally!&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;triggering-the-build-from-the-git-repo&#34;&gt;Triggering the build from the git repo&lt;/h2&gt;

&lt;p&gt;Many approaches to building a project automatically on each commit rely on the CI server, in this case Jenkins, polling the source control system at some pre-defined interval.&lt;/p&gt;

&lt;p&gt;However, if the geo-political events of 2016 are any guide, polling doesn&amp;rsquo;t always lead to the outcome we expect, so in this case we&amp;rsquo;ll create a git &amp;ldquo;hook&amp;rdquo; to trigger the build on every commit to master.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&#34;https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks&#34; title=&#34;Link to git hooks chapter of git book&#34;&gt;git hook&lt;/a&gt; is a script that runs in response to a specific event that occurs in git. There are a bunch of events that can fire hooks, these can be used to enforce commit message conventions, run static code analysis, prevent force-pushes, and all kinds of other arbitrary busywork. The event that&amp;rsquo;s of interest here is the &lt;code&gt;post-commit&lt;/code&gt; hook, which runs &lt;em&gt;after&lt;/em&gt; a commit has been added. In &amp;ldquo;Real Life&amp;rdquo;, hooks are normally added to the &lt;em&gt;server&lt;/em&gt; copy of the repo but since there&amp;rsquo;s no remote in our setup, everything has to be local.&lt;/p&gt;

&lt;p&gt;Notably, even though we&amp;rsquo;re on Windows in this example, hooks are executed by the &lt;code&gt;msysgit&lt;/code&gt; subsystem so much of the usual UNIX shell stuff will work.&lt;/p&gt;

&lt;p&gt;Hooks are stored in the &lt;code&gt;.git&lt;/code&gt; folder in our repo (this is usually hidden on Windows) in files that match the name of the hook. So, our file is called &lt;code&gt;post-commit&lt;/code&gt; (no extension) and contains the &lt;code&gt;#!/bin/sh&lt;/code&gt; &amp;ldquo;shebang&amp;rdquo; plus a single line that uses &lt;code&gt;curl&lt;/code&gt; to hit a specific URL in the Jenkins server that will cause our Jenkins job to poll its source repo as defined in the build pipeline (i.e. &lt;code&gt;file:///C:/Projects/Chinook.JenkinsDemo&lt;/code&gt; )&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;C&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;\Projects\Chinook.JenkinsDemo\.git\hooks&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&amp;gt;type post-commit&lt;/span&gt;
#!/bin/sh
curl http://localhost:8080/git/notifyCommit?url=file:///C:/Projects/Chinook.JenkinsDemo&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note for &lt;code&gt;git&lt;/code&gt; beginners: this file isn&amp;rsquo;t in the repo when you download it, you&amp;rsquo;ll need to add it yourself!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, all that remains is to make an arbitrary change to our project and add a commit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-ssdt-git/changed-table-with-commit-dialog.PNG&#34; alt=&#34;Visual Studio showing changed table and commit dialog&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If everything is working the pipeline job should be triggered, and the new build will also log that it was &amp;ldquo;Started by an SCM Change&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/jenkins-ssdt-git/build-started-by-scm-change.PNG&#34; alt=&#34;Build Triggered by SCM Change&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To clarify what&amp;rsquo;s happened here, we made a change and committed it to our local repo, the &lt;code&gt;post-commit&lt;/code&gt; hook called the url in Jenkins, and Jenkins then &amp;ldquo;polled&amp;rdquo; the git repo, found a new commit, and triggered the pipeline job.&lt;/p&gt;

&lt;p&gt;Until next time, happy triggering!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you commit the change from the &lt;code&gt;git&lt;/code&gt; command line, you may see output along the lines of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git commit -am &amp;quot;add shoesize to artist table&amp;quot;
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   135  100   135    0     0   9000      0 --:--:-- --:--:-- --:--:--  9000Scheduled polling of BuildDeploySsdtFromLocalRepo
No Git consumers using SCM API plugin for: file:///C:/Projects/Chinook.JenkinsDemo

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &amp;ldquo;error&amp;rdquo; message is nothing to worry about, as long as the line &amp;ldquo;Scheduled Polling of &lt;em&gt;myJobName&lt;/em&gt; is present then everything is hunky-dory!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;https://blogs.msdn.microsoft.com/visualstudio/2013/07/24/msbuild-is-now-part-of-visual-studio/&#34;&gt;MSBuild is now part of Visual Studio!&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;&lt;a href=&#34;https://blogs.msdn.microsoft.com/dataaccesstechnologies/2010/01/29/testing-connection-to-sql-server-from-a-service-running-under-local-system-account/&#34;&gt;Testing connection to SQL Server from a service running under Local System Account&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Recovering from the &#34;La-La-Land Moment&#34;</title>
          <link>http://arapaima.uk/post/2017-03-16-inserting-marked-transaction-ssdt-deployment/</link>
          <pubDate>Thu, 16 Mar 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-03-16-inserting-marked-transaction-ssdt-deployment/</guid>
          <description>

&lt;h1 id=&#34;how-ssdt-can-help-with-restoring-a-sql-server-database-to-just-before-that-last-deployment&#34;&gt;How SSDT can help with restoring a SQL Server database to &amp;ldquo;just before that last deployment&amp;rdquo;&lt;/h1&gt;

&lt;p&gt;For as long as I can remember, SSDT and its predecessors have had the option to &amp;ldquo;Back up database before deployment&amp;rdquo;, currently available in the &amp;ldquo;Advanced Publish Settings&amp;rdquo; dialog, among other places. Regrettably, I&amp;rsquo;ve never really had much use for this particular option. Whilst restoring from backup might be a valid strategy for recovering from some kinds of deployment disaster, this could add a great deal of time to the deployment process, assuming a database of non-trivial size.&lt;/p&gt;

&lt;p&gt;In any case, there are really only two kinds of database - the ones where you don&amp;rsquo;t care about the data, such as development and test environments, and the ones (one?) where you do. In the former case, backups don&amp;rsquo;t matter anyway, and in the latter case it is to be hoped that there is already some kind of backup solution in place, ideally managed by someone who isn&amp;rsquo;t you.&lt;/p&gt;

&lt;p&gt;Further, the &amp;ldquo;Backup database before deployment&amp;rdquo; option appears not to offer much control over &lt;em&gt;how&lt;/em&gt; the database is backed up. The backup will be created in the default backup directory of the instance, and will be a &lt;em&gt;full&lt;/em&gt; rather than a &lt;em&gt;copy only&lt;/em&gt; backup, which has the potential to endanger the &amp;ldquo;real&amp;rdquo; disaster recovery plans in the event that these include differential database backups, as the SSDT-generated backup will reset the differential base, meaning &lt;em&gt;this&lt;/em&gt; backup, which the DBAs don&amp;rsquo;t even know about,  will be necessary, along with the subsequent differentials, in case the database needs to be restored.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s likely that the &amp;ldquo;real&amp;rdquo; backup strategy will involve some combination of full, differential, and transaction log backups, and if you&amp;rsquo;re especially lucky will be managed by some kind of &amp;ldquo;Enterprise Backup Solution&amp;rdquo;, which will allow rapid restores to any point in time, just as soon as you figure out where it is that the &amp;ldquo;Enterprise Backup Solution&amp;rdquo; keeps the backup files for your database.&lt;/p&gt;

&lt;p&gt;So, assuming the worst case scenario has arisen, your newly deployed release has been writing rubbish data to the database for the last few hours, the &amp;ldquo;war room&amp;rdquo; has been convened and someone you&amp;rsquo;ve never heard of called the &amp;ldquo;Problem Management Executive Consultant&amp;rdquo; has decided that the right course of action is to &amp;ldquo;Restore the database to immediately before the deployment&amp;rdquo;, how are you to decide when this was?&lt;/p&gt;

&lt;p&gt;Well, hopefully you know from your release logs what time the database was deployed, but what if there was a better way? And what if you aren&amp;rsquo;t sure what timezone the deployment server is in or whether it uses daylight savings time? Well, one solution to this involves a somewhat neglected feature of SQL Server known as &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/ms188929.aspx#Anchor_3&#34;&gt;Marked Transactions&lt;/a&gt;. (See, it doesn&amp;rsquo;t even get its own page in MSDN!)&lt;/p&gt;

&lt;h2 id=&#34;marked-transactions&#34;&gt;Marked Transactions&lt;/h2&gt;

&lt;p&gt;The syntax &lt;code&gt;BEGIN TRANSACTION&lt;/code&gt; &lt;em&gt;tran-name&lt;/em&gt; &lt;code&gt;WITH MARK &#39;Mark Description&#39;&lt;/code&gt; will record the name of the transaction in the transaction log, along with the date, time, LSN, etc. The description is optional, but that gets saved too.&lt;/p&gt;

&lt;p&gt;We can see this in action using the &lt;a href=&#34;https://technet.microsoft.com/en-us/library/ms143221.aspx&#34;&gt;&lt;code&gt;pubs&lt;/code&gt;&lt;/a&gt; database, which if you&amp;rsquo;re younger than a certain age, sadly isn&amp;rsquo;t what you think.&lt;/p&gt;

&lt;p&gt;Note that if you&amp;rsquo;re playing along at home, there are a couple of extra considerations; the database needs to be using the full (or bulk-logged) recovery model and &lt;em&gt;a full backup must already have been taken&lt;/em&gt;, i.e. the database is not in the &lt;a href=&#34;http://www.sqlskills.com/blogs/paul/new-script-is-that-database-really-in-the-full-recovery-mode/&#34;&gt;pseudo-simple&lt;/a&gt; recovery model.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;n&#34;&gt;USE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pubs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;BEGIN&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TRANSACTION&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;update_auths&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MARK&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;update authors entry&amp;#39;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;UPDATE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;authors&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;set&lt;/span&gt; 
&lt;span class=&#34;n&#34;&gt;phone&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;408 496-7223&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;au_id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;172-32-1176&amp;#39;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;COMMIT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TRANSACTION&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we take a peek at the transaction log we can see our marked transaction there:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Transaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;Current&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LSN&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Transaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;Operation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;Description&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Transaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;sp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;name&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;fn_dblog&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;AS&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;
        &lt;span class=&#34;k&#34;&gt;JOIN&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;server_principals&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sp&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;ON&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Transaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sid&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt;   &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;update_auths&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Transaction ID&lt;/th&gt;
&lt;th&gt;Current LSN&lt;/th&gt;
&lt;th&gt;Transaction Name&lt;/th&gt;
&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Transaction SID&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0000:000004ee&lt;/td&gt;
&lt;td&gt;00000022:000005f2:0001&lt;/td&gt;
&lt;td&gt;update_auths&lt;/td&gt;
&lt;td&gt;LOP_BEGIN_XACT&lt;/td&gt;
&lt;td&gt;2017/03/16 18:09:58:327;update_auths;0x0105000000000005150000004cca9a3fa9173a6eba0c5dc9e9030000&lt;/td&gt;
&lt;td&gt;0x0105000000000005150000004CCA9A3FA9173A6EBA0C5DC9E9030000&lt;/td&gt;
&lt;td&gt;ARAPAIMA\Arapaima&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The &lt;code&gt;logmarkhistory&lt;/code&gt; table in &lt;code&gt;msdb&lt;/code&gt; also stores a list of our marked transactions:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;msdb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logmarkhistory&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;database_name&lt;/th&gt;
&lt;th&gt;mark_name&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;user_name&lt;/th&gt;
&lt;th&gt;lsn&lt;/th&gt;
&lt;th&gt;mark_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pubs&lt;/td&gt;
&lt;td&gt;update_auths&lt;/td&gt;
&lt;td&gt;update authors entry&lt;/td&gt;
&lt;td&gt;ARAPAIMA\Arapaima&lt;/td&gt;
&lt;td&gt;34000000152200003&lt;/td&gt;
&lt;td&gt;2017-03-16 18:09:58.327&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So, it seems we can use this functionality to give us a named point in the transaction log to restore to in the event of career-threatening disaster.&lt;/p&gt;

&lt;p&gt;The thing to note is that we need to update &lt;em&gt;something&lt;/em&gt; in the database where we&amp;rsquo;re creating the mark, but it doesn&amp;rsquo;t really matter &lt;em&gt;what&lt;/em&gt; we update. It&amp;rsquo;s often useful to have a &lt;code&gt;Releases&lt;/code&gt; table in our databases where we can store the version number, date, and other fun facts about our database, so this is as good a candidate as any.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;k&#34;&gt;CREATE&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Releases&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;VARCHAR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NOT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;PRIMARY&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;KEY&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
        &lt;span class=&#34;n&#34;&gt;DateDeployed&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DATETIME2&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NOT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;DEFAULT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SYSUTCDATETIME&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; 
        &lt;span class=&#34;c1&#34;&gt;--Not taking any chances with the time zone!
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We need to create this table and add it to our database project.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve created a proc to do the inserting:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;k&#34;&gt;CREATE&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;PROCEDURE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spFireInTheHole&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
	&lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;varchar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;AS&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;BEGIN&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TRANSACTION&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MARK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt; 
  &lt;span class=&#34;k&#34;&gt;INSERT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;INTO&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Releases&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;VALUES&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;COMMIT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TRANSACTION&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;RETURN&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And now for the tricky part&amp;hellip;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;first&lt;/em&gt; time we deploy the database project after adding these objects, they won&amp;rsquo;t be present in the target database, so we won&amp;rsquo;t be able to call them from the pre-deployment script. This means we need to wrap them with &lt;code&gt;IF EXISTS&lt;/code&gt; in the pre-deployment script so that they don&amp;rsquo;t get called if they don&amp;rsquo;t exist. The drawback of this approach, naturally, is that &amp;ldquo;Release 0.0.1&amp;rdquo; won&amp;rsquo;t be recorded in your database for posterity. In my experience of such matters, the gap between Release 0.0.1 and Release 0.0.2 is normally measured in minutes rather than years, so I&amp;rsquo;m not particularly concerned about this.&lt;/p&gt;

&lt;p&gt;Finally, we are going to use a &lt;code&gt;sqlcmd&lt;/code&gt; &lt;em&gt;variable&lt;/em&gt; to hold the release number; this means we can pass it in at deployment time. Most deployment tools know how to do this, for this example we&amp;rsquo;ll just pass it on the command line.&lt;/p&gt;

&lt;h3 id=&#34;the-pre-deployment-script&#34;&gt;The pre-deployment script&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;cm&#34;&gt;/*
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt; Pre-Deployment Script Template							
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;--------------------------------------------------------------------------------------
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt; This file contains SQL statements that will be executed before the build script.	
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt; Use SQLCMD syntax to include a file in the pre-deployment script.			
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt; Example:      :r .\myfile.sql								
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt; Use SQLCMD syntax to reference a variable in the pre-deployment script.		
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt; Example:      :setvar TableName MyTable							
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;               SELECT * FROM [$(TableName)]					
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;--------------------------------------------------------------------------------------
&lt;/span&gt;&lt;span class=&#34;cm&#34;&gt;*/&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;IF&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;EXISTS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tables&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;where&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Releases&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; 
&lt;span class=&#34;k&#34;&gt;AND&lt;/span&gt; 
&lt;span class=&#34;k&#34;&gt;EXISTS&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;procedures&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NAME&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;spFireInTheHole&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;BEGIN&lt;/span&gt;
	
        &lt;span class=&#34;k&#34;&gt;EXEC&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spFireInTheHole&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ReleaseVersion&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;$(ReleaseVersion)&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;END&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, after we build our &lt;code&gt;dacpac&lt;/code&gt;, we supply the version number at deploy time:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sqlpackage.exe /Action:Publish /SourceFile:pubs.dacpac /TargetServerName:(local) /TargetDatabaseName:pubs /v:ReleaseVersion=0.2.0

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we run a few &amp;ldquo;Releases&amp;rdquo;, we can see the marks building up in &lt;code&gt;msdb..logmarkhistory&lt;/code&gt;:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;database name&lt;/th&gt;
&lt;th&gt;mark name&lt;/th&gt;
&lt;th&gt;description&lt;/th&gt;
&lt;th&gt;user name&lt;/th&gt;
&lt;th&gt;lsn&lt;/th&gt;
&lt;th&gt;mark_time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pubs&lt;/td&gt;
&lt;td&gt;0.0.2&lt;/td&gt;
&lt;td&gt;0.0.2&lt;/td&gt;
&lt;td&gt;ARAPAIMA\Arapaima&lt;/td&gt;
&lt;td&gt;34000000172100002&lt;/td&gt;
&lt;td&gt;2017-03-16 20:06:40.490&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;pubs&lt;/td&gt;
&lt;td&gt;0.0.3&lt;/td&gt;
&lt;td&gt;0.0.3&lt;/td&gt;
&lt;td&gt;ARAPAIMA\Arapaima&lt;/td&gt;
&lt;td&gt;34000000172200003&lt;/td&gt;
&lt;td&gt;2017-03-16 20:07:14.673&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;pubs&lt;/td&gt;
&lt;td&gt;0.0.3&lt;/td&gt;
&lt;td&gt;0.0.3&lt;/td&gt;
&lt;td&gt;ARAPAIMA\Arapaima&lt;/td&gt;
&lt;td&gt;34000000172300002&lt;/td&gt;
&lt;td&gt;2017-03-16 20:08:16.183&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;pubs&lt;/td&gt;
&lt;td&gt;0.0.4&lt;/td&gt;
&lt;td&gt;0.0.4&lt;/td&gt;
&lt;td&gt;ARAPAIMA\Arapaima&lt;/td&gt;
&lt;td&gt;34000000172400003&lt;/td&gt;
&lt;td&gt;2017-03-16 20:08:39.760&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In this particular setup, you&amp;rsquo;ll get an error if you try to supply the same version number more than once, as &lt;code&gt;ReleaseVersion&lt;/code&gt; is the primary key of the &lt;code&gt;Releases&lt;/code&gt; table.&lt;/p&gt;

&lt;h3 id=&#34;disaster-strikes&#34;&gt;Disaster Strikes&lt;/h3&gt;

&lt;p&gt;So, the career-threatening disaster has happened, and we need to restore our database to its state immediately prior to release 0.6.0. If there hasn&amp;rsquo;t been a log backup since the deployment, the first step is to make one.&lt;/p&gt;

&lt;p&gt;Then, with a bit of DBA magic, we can restore our database to just before the deployment started. I generated these scripts from SQL Server Management Studio, unless you&amp;rsquo;ve got a competent adult handy I suggest you do the same. The crucial bit is in the last line where we specify &lt;code&gt;STOPBEFOREMARK&lt;/code&gt;, which is exactly what we want to do.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span class=&#34;n&#34;&gt;USE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;master&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;BACKUP&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LOG&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pubs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;TO&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;DISK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;C:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\Backup\pubs_LogBackup_2017-03-16_21-05-56.bak&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NOFORMAT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NOINIT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NAME&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;pubs_LogBackup_2017-03-16_21-05-56&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NOSKIP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NOREWIND&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NOUNLOAD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NORECOVERY&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;STATS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;RESTORE&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;DATABASE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pubs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;DISK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;C:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\Backup\pubs.bak&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;FILE&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NORECOVERY&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NOUNLOAD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;STATS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;RESTORE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LOG&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pubs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;DISK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;C:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\Backup\pubs.bak&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;FILE&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NORECOVERY&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NOUNLOAD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;STATS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;RESTORE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LOG&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pubs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;DISK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;C:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\Backup\pubs.bak&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;FILE&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NORECOVERY&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NOUNLOAD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;STATS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;RESTORE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LOG&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pubs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;DISK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;C:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\Backup\pubs.bak&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WITH&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;FILE&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;NOUNLOAD&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;STATS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;  &lt;span class=&#34;n&#34;&gt;STOPBEFOREMARK&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;0.6.0&amp;#39;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;AFTER&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;2017-03-16T20:46:07&amp;#39;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And with that, the database is restored to its state immediately prior to the deployment - you can check this by looking at the &lt;code&gt;Releases&lt;/code&gt; table - , and all that is left to do is blame the guy that left last month for the &amp;ldquo;rogue code&amp;rdquo; that &amp;ldquo;crept&amp;rdquo; into the release. This is normally possible by editing old git commit messages and force pushing the master branch, but that&amp;rsquo;s a topic for another day.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>A script to download and install the Pester testing framework for PowerShell</title>
          <link>http://arapaima.uk/post/2017-02-28-script-to-download-pester/</link>
          <pubDate>Tue, 28 Feb 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-02-28-script-to-download-pester/</guid>
          <description>

&lt;p&gt;I finally got around to looking into &lt;a href=&#34;https://github.com/pester/Pester&#34;&gt;Pester&lt;/a&gt; for test-driven development with PowerShell. In case it&amp;rsquo;s useful to anyone, and so that I don&amp;rsquo;t lose it, I created a script to download the latest release version, store it in the user&amp;rsquo;s PowerShell modules folder, and run a quick test to make sure everything is working. The script is stored in a &lt;a href=&#34;https://gist.github.com/gavincampbell/05c803654ff70d21d538b49f0e363a6a&#34;&gt;GitHubGist&lt;/a&gt; and is reproduced below. Some of the cmdlets require a fairly recent version of PowerShell, possibly even 5.0. The steps are roughly as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Figure out what the latest release number is.&lt;/li&gt;
&lt;li&gt;Assemble the correct url to download the zip file.&lt;/li&gt;
&lt;li&gt;Download and &amp;ldquo;Unblock&amp;rdquo; the zip file.&lt;/li&gt;
&lt;li&gt;Figure out where the user&amp;rsquo;s modules folder is and create it if it doesn&amp;rsquo;t exist. I had to exclude the &lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;vscode&lt;/a&gt; modules folder for this to work for me.&lt;/li&gt;
&lt;li&gt;Unzip the downloaded file to the right place.&lt;/li&gt;
&lt;li&gt;Import the newly installed modules.&lt;/li&gt;
&lt;li&gt;Scaffold a test with &lt;code&gt;New-Fixture&lt;/code&gt;. The name is randomised to minimise the chance of overwriting something that&amp;rsquo;s already there.&lt;/li&gt;
&lt;li&gt;Hack the generated files about a bit.&lt;/li&gt;
&lt;li&gt;Run the test.&lt;/li&gt;
&lt;li&gt;Clean up the generated files.&lt;/li&gt;
&lt;li&gt;Profit!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-script&#34;&gt;The script&lt;/h2&gt;

&lt;script src=&#34;//gist.github.com/gavincampbell/05c803654ff70d21d538b49f0e363a6a.js&#34;&gt;&lt;/script&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Testing whether a branch exists before checking out in Jenkins Pipeline</title>
          <link>http://arapaima.uk/post/2017-02-15-jenkins-pipeline-checkout-branch-if-exists/</link>
          <pubDate>Fri, 17 Feb 2017 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2017-02-15-jenkins-pipeline-checkout-branch-if-exists/</guid>
          <description>

&lt;p&gt;For reasons, I recently found myself in a scenario where I needed to test whether a branch existed before checking it out, and resorting to a sensible default - such as checking out &lt;code&gt;master&lt;/code&gt;, if it didn&amp;rsquo;t. From the command line, this is a simple matter of &lt;code&gt;git branch -l | grep myBranch&lt;/code&gt;, but I needed to do this from the context of a Jenkins pipeline job.&lt;/p&gt;

&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h2&gt;

&lt;p&gt;For simplicity, I&amp;rsquo;m creating a local repo I can point my Jenkins job at, right inside the &lt;code&gt;JENKINS_HOME&lt;/code&gt; folder.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;jenkins@d99f4f77acdb:/$ &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;
jenkins@d99f4f77acdb:~$ mkdir testBranch &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; testBranch
jenkins@d99f4f77acdb:~/testBranch$ git init
Initialized empty Git repository in /var/jenkins_home/testBranch/.git/
jenkins@d99f4f77acdb:~/testBranch$ &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Half a league, half a league, Half a league onward.&amp;#34;&lt;/span&gt; &amp;gt; myFile.txt
jenkins@d99f4f77acdb:~/testBranch$ git add .
jenkins@d99f4f77acdb:~/testBranch$ git commit -m &lt;span class=&#34;s2&#34;&gt;&amp;#34;Initial Commit&amp;#34;&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;master &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;root-commit&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; 7ef4b3b&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Initial Commit
 &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; file changed, &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; insertion&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
 create mode &lt;span class=&#34;m&#34;&gt;100644&lt;/span&gt; myFile.txt
jenkins@d99f4f77acdb:~/testBranch$ git checkout -b thisBranchExists
Switched to a new branch &lt;span class=&#34;s1&#34;&gt;&amp;#39;thisBranchExists&amp;#39;&lt;/span&gt;
jenkins@d99f4f77acdb:~/testBranch$ &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;All in the valley of Death Rode the six hundred&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; myFile.txt
jenkins@d99f4f77acdb:~/testBranch$ git commit -am &lt;span class=&#34;s2&#34;&gt;&amp;#34;a change&amp;#34;&lt;/span&gt;
jenkins@d99f4f77acdb:~/testBranch$ git checkout master
Switched to branch &lt;span class=&#34;s1&#34;&gt;&amp;#39;master&amp;#39;&lt;/span&gt;
jenkins@d99f4f77acdb:~/testBranch$ git log --oneline --decorate --all --graph
* 550587f &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;thisBranchExists&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; a change
* 7ef4b3b &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;HEAD, master&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; Initial Commit&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With that out of the way, we have a repo with two branches, one of which is a single commit ahead of the other. Our goal, in the pipeline script, is to checkout the &lt;code&gt;thisBranchExists&lt;/code&gt; branch, if it exists, or to fall back to &lt;code&gt;master&lt;/code&gt; if it doesn&amp;rsquo;t.&lt;/p&gt;

&lt;h2 id=&#34;the-pipeline-script&#34;&gt;The Pipeline script&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve created a simple script in the Pipeline UI to demonstrate the use of &lt;a href=&#34;https://jenkins.io/doc/pipeline/steps/workflow-multibranch/#resolvescm-resolves-an-scm-from-an-scm-source-and-a-list-of-candidate-target-branch-names&#34;&gt;&lt;code&gt;resolveScm&lt;/code&gt;&lt;/a&gt; to figure out whether the branch we want exists. &lt;code&gt;resolveScm&lt;/code&gt; will go through the contents of &lt;code&gt;targets&lt;/code&gt; in order, and return the first one that matches a branch name in the repo. We then checkout the branch and print the contents of the file to the console with &lt;code&gt;cat&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-groovy&#34; data-lang=&#34;groovy&#34;&gt;&lt;span class=&#34;n&#34;&gt;node&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;kt&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;resolveScm&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;source:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;$class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;GitSCMSource&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;credentialsId:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;nl&#34;&gt;excludes:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;id:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;ignoreOnPushNotifications:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;includes:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; 
    &lt;span class=&#34;nl&#34;&gt;remote:&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;file://$JENKINS_HOME/testBranch&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;targets:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;thisBranchExists&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;master&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;checkout&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;sh&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;cat myFile.txt&amp;#39;&lt;/span&gt;
   
&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;the-happy-path&#34;&gt;The Happy path&lt;/h2&gt;

&lt;p&gt;When we run this job we get the following output, indicating that our branch was located successfully. The contents of the file after the second commit are printed to the console.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Running on master in /var/jenkins_home/workspace/checkout_branch_if_exists
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; resolveScm
Checking &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; first existing branch from &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;thisBranchExists, master&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;...
 &amp;gt; git rev-parse --is-inside-work-tree &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Setting origin to file://&lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;/testBranch
 &amp;gt; git config remote.origin.url file://&lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;/testBranch &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt; pruning origin...
Fetching upstream changes from origin
 &amp;gt; git --version &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/* --prune
Getting remote branches...
Seen branch in repository origin/master
Seen branch in repository origin/thisBranchExists
Seen &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; remote branches
Checking branch master
Checking branch thisBranchExists
Found thisBranchExists at revision 550587fa43778989f50ca314feea38fdee50b21c
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; checkout
 &amp;gt; git rev-parse --is-inside-work-tree &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching changes from the remote Git repository
 &amp;gt; git config remote.origin.url file:///var/jenkins_home/testBranch &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching upstream changes from file:///var/jenkins_home/testBranch
 &amp;gt; git --version &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git fetch --tags --progress file:///var/jenkins_home/testBranch +refs/heads/*:refs/remotes/origin/*
Checking out Revision 550587fa43778989f50ca314feea38fdee50b21c &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;thisBranchExists&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
 &amp;gt; git config core.sparsecheckout &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git checkout -f 550587fa43778989f50ca314feea38fdee50b21c
 &amp;gt; git rev-list 550587fa43778989f50ca314feea38fdee50b21c &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; sh
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;checkout_branch_if_exists&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Running shell script
+ cat myFile.txt
Half a league, half a league, Half a league onward.
All in the valley of Death Rode the six hundred
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; // node
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; End of Pipeline
Finished: SUCCESS&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;the-less-happy-path&#34;&gt;The &amp;ldquo;less-happy&amp;rdquo; path&lt;/h2&gt;

&lt;p&gt;Lets see what happens if we change our list of &lt;code&gt;targets&lt;/code&gt; to &lt;code&gt;[&#39;thisBranchDoesNotExist&#39;, &#39;master&#39;]&lt;/code&gt;. This time we don&amp;rsquo;t find our &amp;ldquo;preferred&amp;rdquo; branch, so we fall back to &lt;code&gt;master&lt;/code&gt; and print the contents of the file as it was after the first commit.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; node
Running on master in /var/jenkins_home/workspace/checkout_branch_if_exists
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; resolveScm
Checking &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; first existing branch from &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;thisBranchDoesNotExist, master&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;...
 &amp;gt; git rev-parse --is-inside-work-tree &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Setting origin to file://&lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;/testBranch
 &amp;gt; git config remote.origin.url file://&lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;/testBranch &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt; pruning origin...
Fetching upstream changes from origin
 &amp;gt; git --version &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/* --prune
Getting remote branches...
Seen branch in repository origin/master
Seen branch in repository origin/thisBranchExists
Seen &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; remote branches
Checking branch thisBranchExists
Checking branch master
Done.
Found master at revision 7ef4b3b3eaf296528e47cdf8946545b38f49fb95
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; checkout
 &amp;gt; git rev-parse --is-inside-work-tree &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching changes from the remote Git repository
 &amp;gt; git config remote.origin.url file:///var/jenkins_home/testBranch &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching upstream changes from file:///var/jenkins_home/testBranch
 &amp;gt; git --version &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git fetch --tags --progress file:///var/jenkins_home/testBranch +refs/heads/*:refs/remotes/origin/*
Checking out Revision 7ef4b3b3eaf296528e47cdf8946545b38f49fb95 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;master&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
 &amp;gt; git config core.sparsecheckout &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git checkout -f 7ef4b3b3eaf296528e47cdf8946545b38f49fb95
 &amp;gt; git rev-list 550587fa43778989f50ca314feea38fdee50b21c &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; sh
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;checkout_branch_if_exists&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Running shell script
+ cat myFile.txt
Half a league, half a league, Half a league onward.
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; // node
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; End of Pipeline
Finished: SUCCESS&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;the-unhappy-path&#34;&gt;The Unhappy path&lt;/h2&gt;

&lt;p&gt;Finally, let&amp;rsquo;s see what happens if none of the branches we specify in &lt;code&gt;targets&lt;/code&gt; exist in the repo. Regrettably, the output is rather shorter, as the step fails if no matching branches are found.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Running on master in /var/jenkins_home/workspace/checkout_branch_if_exists
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; resolveScm
Checking &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; first existing branch from &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;thisBranchDoesNotExist, thisBranchDoesNotExistEither&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;...
 &amp;gt; git rev-parse --is-inside-work-tree &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Setting origin to file://&lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;/testBranch
 &amp;gt; git config remote.origin.url file://&lt;span class=&#34;nv&#34;&gt;$JENKINS_HOME&lt;/span&gt;/testBranch &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;Fetching &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt; pruning origin...
Fetching upstream changes from origin
 &amp;gt; git --version &lt;span class=&#34;c1&#34;&gt;# timeout=10
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt; &amp;gt; git fetch --tags --progress origin +refs/heads/*:refs/remotes/origin/* --prune
Getting remote branches...
Seen branch in repository origin/master
Seen branch in repository origin/thisBranchExists
Seen &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; remote branches
Checking branch master
Checking branch thisBranchExists
Done.
Could not find any matching branch%n
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; // node
&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Pipeline&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; End of Pipeline
ERROR: Could not find any matching branch
Finished: FAILURE&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
      
    
      
        <item>
          <title>What&#39;s in a name?</title>
          <link>http://arapaima.uk/post/2016-11-12-database-refactoring-ssdt-renaming-objects/</link>
          <pubDate>Sat, 12 Nov 2016 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2016-11-12-database-refactoring-ssdt-renaming-objects/</guid>
          <description>

&lt;p&gt;Continuing our horticultural theme, in this article we&amp;rsquo;ll look at the built-in support in SSDT for renaming database objects including tables, columns, and programmable objects, as well as peering into the details of how these changes are managed at deployment time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/66/Rosa_laxa.jpg&#34; alt=&#34;That which we call a rose. By any other name would smell as sweet&#34; title=&#34;That which we call a rose. By any other name would smell as sweet&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;renaming-columns&#34;&gt;Renaming columns&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Refactoring Databases, p 109&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;the-easy-way&#34;&gt;The easy way&lt;/h3&gt;

&lt;p&gt;We can rename a column just by right-clicking in the &lt;code&gt;CREATE TABLE&lt;/code&gt; script and selecting Refactor &amp;rarr; Rename.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/RefactorRenameSelected.PNG&#34; alt=&#34;Renaming the InvoiceId Column by right-clicking in the editor window&#34; title=&#34;Renaming the InvoiceId Column by right-clicking in the editor window&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Under normal circumstances, renaming the Primary Key column of a table such as &amp;ldquo;Invoices&amp;rdquo; would be a recipe for disaster, but SSDT can help to ease such changes by automatically updating all references to the column to use the new name. In this case we are renaming the column InvoiceId to Invoice_Id, and by specifying the option to preview the changes, we can see a list of all the objects that reference this column by its old name.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/rename%20column%20preview.PNG&#34; alt=&#34;SSDT shows a preview of which objects will be updated to refer to the new name&#34; title=&#34;SSDT shows a preview of which objects will be updated to refer to the new name&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s something of note here, which is that &lt;em&gt;only&lt;/em&gt; the InvoiceId column from the Invoices table is being renamed, any other columns called InvoiceId (such as the one in the InvoiceLine table) are unaffected. The foreign key constraint on that particular column, however, &lt;em&gt;is&lt;/em&gt; updated to use the new name of the referenced column.&lt;/p&gt;

&lt;p&gt;What this demonstrates is that there is something more than global search and replace going on here; SSDT is using its in-memory model of the database to determine which changes need to be made&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-refactorlog&#34;&gt;The refactorlog&lt;/h3&gt;

&lt;p&gt;When we click apply, two things happen. The first is that all the references to this column are updated to use the new name. The second is that a new file appears in the solution, with the extension &lt;code&gt;.refactorlog&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;utf-8&amp;#34;?&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;Operations&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Version=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;1.0&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;xmlns=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;http://schemas.microsoft.com/sqlserver/dac/Serialization/2012/02&amp;#34;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Operation&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Rename Refactor&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Key=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;209f3afd-7195-401f-853f-aa3a906d39db&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;ChangeDateTime=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;11/08/2016 20:02:31&amp;#34;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo].[Invoice].[InvoiceId]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlSimpleColumn&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ParentElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo].[Invoice]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ParentElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlTable&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;NewName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[Invoice_Id]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;/Operation&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/Operations&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This file is how &lt;code&gt;sqlpackage.exe&lt;/code&gt; (or SSDT publish, or DacFX.Deploy) will detemine &lt;em&gt;at deploy time&lt;/em&gt; that we are renaming this column from InvoiceID to Invoice_Id rather than dropping the InvoiceID column and creating a new column called Invoice_ID. We can see in the XML that this is specifying the column and table name, and the precise action to perform. This is known, in the jargon, as &amp;ldquo;preserving the intent&amp;rdquo; of the refactoring. If we build a project containing a &lt;code&gt;.refactorlog&lt;/code&gt; file and examine the resulting &lt;code&gt;.dacpac&lt;/code&gt;, we can see that the regular &lt;code&gt;.dacpac&lt;/code&gt; contents have been joined by a &lt;code&gt;refactor.xml&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;$ unzip -l Refactoring.Chinook.dacpac
Archive:  Refactoring.Chinook.dacpac
  Length      Date    Time    Name
---------  ---------- -----   ----
    69711  2016-11-10 05&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;44&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;   model.xml&lt;/span&gt;
      606  2016-11-10 05&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;44&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;   refactor.xml&lt;/span&gt;
      203  2016-11-10 05&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;44&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;   DacMetadata.xml&lt;/span&gt;
     1118  2016-11-10 05&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;44&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;   Origin.xml&lt;/span&gt;
      175  2016-11-10 05&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;44&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;   [Content_Types].xml&lt;/span&gt;
---------                     -------
    71813                     5 files&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The contents of this file are the same as the &lt;code&gt;.refactorlog&lt;/code&gt; file from our solution.&lt;/p&gt;

&lt;p&gt;When we publish the project we see the following output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bat&#34; data-lang=&#34;bat&#34;&gt;The following operation was generated from a refactoring log file 209f3afd-7195-401f-853f-aa3a906d39db
&lt;span class=&#34;k&#34;&gt;Rename&lt;/span&gt; [dbo].[Invoice].[InvoiceId] to Invoice_Id
Caution: Changing any part of an object name could break scripts and stored procedures.

Altering [dbo].[InvoicesWithLineTotals]...
Altering [dbo].[UpdateInvoiceBillingAddress]...
Update complete.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The publish action has read the &lt;code&gt;refactorlog&lt;/code&gt; file and taken the appropriate action. In addition, the &amp;ldquo;key&amp;rdquo; for this refactoring has been stored in a new table in our database called &lt;code&gt;dbo._RefactorLog&lt;/code&gt;, which gets created the first time we deploy a dacpac containing a &lt;code&gt;refactor.xml&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;c1&#34;&gt;-- Refactoring step to update target server with deployed transaction logs
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;IF&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NOT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;EXISTS&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OperationKey&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__RefactorLog&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OperationKey&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;209f3afd-7195-401f-853f-aa3a906d39db&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;INSERT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;INTO&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__RefactorLog&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OperationKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;values&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;209f3afd-7195-401f-853f-aa3a906d39db&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On subsequent deployments, this table is read and any refactorings recorded here are skipped from the deployment.&lt;/p&gt;

&lt;h3 id=&#34;appearances-can-be-deceptive&#34;&gt;Appearances can be deceptive&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s another UI wrinkle here which is worth examining, so we will rename another column, this time using the table designer.&lt;/p&gt;

&lt;p&gt;Another entry has been added to the &lt;code&gt;refactorlog&lt;/code&gt; file, and it appears as if the references to this column elsewhere in the model have been updated (note the red tick showing that &lt;code&gt;PlaylistTrack.sql&lt;/code&gt; and &lt;code&gt;InvoiceLine.sql&lt;/code&gt; have been modified.)
&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/Renaming%20a%20Column%20in%20the%20table%20designer.PNG&#34; alt=&#34;Renaming a column using the table designer&#34; title=&#34;Renaming a column using the table designer&#34; /&gt;
However, when we go to build the project, we get an error:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQL71501: Procedure: [dbo].[ChangeTrackPriceByFactor] has an unresolved reference to object [dbo].[Track].[TrackId].	
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Alternatively, the error will appear in the SSDT UI as soon as the Intellisense catches up). There was a stored procedure referencing this column by name, which is now causing the build to fail (remember that &lt;a href=&#34;http://arapaima.uk/post/2016-10-25-database-refactoring-ssdt-dropping-objects/#dropping-a-table&#34;&gt;deferred name resolution works for table names but not for column names&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is because of a detail of how foreign keys - and primary keys, for that matter - are maintained by SQL Server itself. If we look at the contents of &lt;code&gt;sys.foreign_key_columns&lt;/code&gt;, there isn&amp;rsquo;t a column name in sight (&lt;code&gt;sys.index_columns&lt;/code&gt; looks much the same):&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;constraint_ object_id&lt;/th&gt;
&lt;th&gt;constraint_ column_id&lt;/th&gt;
&lt;th&gt;parent_ object_id&lt;/th&gt;
&lt;th&gt;parent_ column_id&lt;/th&gt;
&lt;th&gt;referenced_ object_id&lt;/th&gt;
&lt;th&gt;referenced_ column_id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1045578763&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;885578193&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;565577053&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;917578307&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;565577053&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;597577167&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;965578478&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;725577623&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;629577281&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;933578364&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;629577281&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;661577395&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If we fix the reference in our stored procedure and go on to generate a publish script, all we see for this change is&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;n&#34;&gt;PRINT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The following operation was generated from a refactoring log file 8a905288-76de-4cc4-aad9-c6dddf081a17&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;PRINT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Rename [dbo].[Track].[TrackId] to Track_Id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;EXECUTE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sp_rename&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;objname&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;[dbo].[Track].[TrackId]&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;newname&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Track_Id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;objtype&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;COLUMN&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;PRINT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Altering [dbo].[ChangeTrackPriceByFactor]...&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;ALTER&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;PROCEDURE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ChangeTrackPriceByFactor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
	&lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrackID&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
	&lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Factor&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;NUMERIC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;AS&lt;/span&gt;
	&lt;span class=&#34;k&#34;&gt;UPDATE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Track&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;SET&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;UnitPrice&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Factor&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Track_Id&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrackID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;RETURN&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;c1&#34;&gt;-- Refactoring step to update target server with deployed transaction logs
&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;IF&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NOT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;EXISTS&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;SELECT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OperationKey&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;FROM&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__RefactorLog&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;WHERE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;OperationKey&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;8a905288-76de-4cc4-aad9-c6dddf081a17&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;INSERT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;INTO&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__RefactorLog&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;OperationKey&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;values&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;8a905288-76de-4cc4-aad9-c6dddf081a17&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;refactorlog&lt;/code&gt; is updated to include this change, but there are no changes deployed to any of the tables that referenced the &lt;code&gt;TrackId&lt;/code&gt; column via foreign keys. This is because foreign keys - as noted above - don&amp;rsquo;t &lt;em&gt;really&lt;/em&gt; use column names, they use object and column ids, so it is sufficient to rename the referenced column with &lt;code&gt;sp_rename&lt;/code&gt;. Stored Procedures, triggers, and other programmable objects, however, &lt;em&gt;do&lt;/em&gt; reference columns and tables by name - in &lt;code&gt;sys.sql_modules&lt;/code&gt; - so these references aren&amp;rsquo;t updated automatically.&lt;/p&gt;

&lt;p&gt;However, SSDT itself takes into consideration that when we rename a column referenced by a foreign (or primary) key, the definition required to create the constraint from scratch will need to be updated, which is why the files containing the referencing tables are all updated.&lt;/p&gt;

&lt;p&gt;This behaviour may seem inconsistent, but it is in fact consistent with the &lt;a href=&#34;https://msdn.microsoft.com/en-gb/library/ms188351.aspx#Anchor_3&#34; title=&#34;MSDN documentation for sp_rename&#34;&gt;behaviour of &lt;code&gt;sp_rename&lt;/code&gt; itself&lt;/a&gt;, which is to say that constraints and indexes aren&amp;rsquo;t broken by &lt;code&gt;sp_rename&lt;/code&gt;, but stored procedures, triggers, etc. are.&lt;/p&gt;

&lt;h3 id=&#34;the-wrong-way&#34;&gt;The wrong way&lt;/h3&gt;

&lt;p&gt;In contrast, renaming a column by editing the Transact-SQL file directly delivers the promised disaster, as SSDT will attempt to drop the column with the old name and create a new column with the new name.&lt;/p&gt;

&lt;p&gt;In the best-case scenario we get a validation error that stops the project from building, assuming the renamed column is referenced by some other object in the project.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/renaming%20a%20key%20column%20in%20the%20sql%20file.PNG&#34; alt=&#34;Renaming a column by editing the .sql file&#34; title=&#34;Renaming a column by editing the .sql file]&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In all other scenarios, the script executed at deploy time is as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;n&#34;&gt;PRINT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Altering [dbo].[Genre]...&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;ALTER&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Genre&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;DROP&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;COLUMN&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;ALTER&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Genre&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;ADD&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GenreName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVARCHAR&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;120&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The only safety net left at this point is the &lt;a href=&#34;http://arapaima.uk/post/2016-10-25-database-refactoring-ssdt-dropping-objects/#pulling-the-trigger-3&#34;&gt;BlockOnPossibleDataLoss&lt;/a&gt; deploy-time option, which is enabled by default. This will stop the deployment from proceeding.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Msg 50000, Level 16, State 127, Line 48
Rows were detected. The schema update is terminating because data loss might occur.
** An error was encountered during execution of batch. Exiting.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;renaming-tables&#34;&gt;Renaming tables&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Refactoring Databases, p 113&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are two options in the refactoring context menu that are relevant to naming tables; &amp;ldquo;Rename&amp;rdquo; and &amp;ldquo;Move to Schema&amp;rdquo;. In some RDBMSs, notably &lt;a href=&#34;https://docs.oracle.com/database/122/CNCPT/tables-and-table-clusters.htm#GUID-72E247B5-F39A-47F1-9445-72D9221F57E3&#34; title=&#34;Introduction to schema objects, Oracle 12.2&#34;&gt;Oracle&lt;/a&gt;, the notion of a schema is tightly coupled to the notion of a user, such that the user account in question &amp;ldquo;owns&amp;rdquo; the tables and other objects contained therein. SQL Server implemented a similar concept prior to SQL Server 2005, when the &lt;a href=&#34;https://technet.microsoft.com/en-us/library/dd283095.aspx&#34; title=&#34;SQL Server Best Practices – Implementation of Database Object Schemas&#34;&gt;link between users and schemas was severed&lt;/a&gt; such that a schema became more like a namespace, or even a filesystem folder, since a schema remains a securable object. Under either analogy - namespace or folder - the name of the schema can be considered to be a part of the (qualified) name of the table, meaning that moving an object to a new schema is merely a special case of renaming.&lt;/p&gt;

&lt;h3 id=&#34;the-right-way&#34;&gt;The right way&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/refactoring-menu.PNG&#34; alt=&#34;Right-click refactor menu for a table&#34; title=&#34;Right-click Refactor menu for a table&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When we rename a table, we get the usual &amp;ldquo;refactor preview&amp;rdquo; showing the changes about to be applied to the project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/Refactor%20renaming%20a%20table.PNG&#34; alt=&#34;Refactor preview for renaming a table&#34; title=&#34;Refactor preview for renaming a table&#34; /&gt;&lt;/p&gt;

&lt;p&gt;On clicking apply, the relevant objects are updated and a new entry is inserted into the &lt;code&gt;refactorlog&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;Operation&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Rename Refactor&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Key=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;cce28c30-adb0-4019-876e-d93cc2ca0011&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;ChangeDateTime=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;11/12/2016 14:22:18&amp;#34;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;gt;&lt;/span&gt;
   &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo].[Artist]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
   &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlTable&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
   &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ParentElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
   &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ParentElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlSchema&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
   &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;NewName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[Artiste]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
 &lt;span class=&#34;nt&#34;&gt;&amp;lt;/Operation&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The process for moving a table between schemas is similar, we are presented with a preview of the changes about to be made:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/move%20to%20schema%20preview.PNG&#34; alt=&#34;Refactor preview for move to schema&#34; title=&#34;Refactor preview for move to schema&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and a new entry is made in the &lt;code&gt;refactorlog&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;Operation&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Move Schema&amp;#34;&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Key=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;985e03c6-37f8-48c8-8ce8-5ed37fbb7c00&amp;#34;&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;ChangeDateTime=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;11/12/2016 14:46:31&amp;#34;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo].[Invoice]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlTable&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;NewSchema&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Sales&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;IsNewSchemaExternal&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/Operation&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, when we come to deploy our change, the table is moved to the new schema and all the referencing objects are updated:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PRINT N&#39;The following operation was generated from a refactoring log file 985e03c6-37f8-48c8-8ce8-5ed37fbb7c00&#39;;

PRINT N&#39;Move object [dbo].[Invoice] to different schema [Sales]&#39;;

GO
ALTER SCHEMA [Sales] TRANSFER [dbo].[Invoice];

GO
ALTER VIEW [dbo].[InvoicesWithLineTotals] 
	AS SELECT I.[Invoice_Id],
	InvoiceTotal
	FROM [Sales].Invoice AS I CROSS APPLY dbo.CalculateInvoiceTotal(I.[Invoice_Id]);
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-wrong-way-1&#34;&gt;The wrong way&lt;/h3&gt;

&lt;p&gt;As with columns, if we rename a table by editing the &lt;code&gt;.sql&lt;/code&gt; file directly, we get a potentially undesirable outcome, namely that a new table will created with the new name, possibly at the expense of the current table. If we are lucky we get a validation error that stops the project from building:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-2/rename%20validation%20error.PNG&#34; alt=&#34;Error from renaming a table in the sql file&#34; title=&#34;error from renaming a table in the .sql file&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we are less lucky we may get some warnings (remember that deferred name resolution means that a missing table is only a warning rather than an error in a stored procedure), but at deploy time we will get a new table created with the new name, and possibly even a &lt;code&gt;DROP TABLE&lt;/code&gt; for the existing table, assuming we have the appropriate options set. (By default, SSDT won&amp;rsquo;t drop objects from the database unless we specify &amp;ldquo;&lt;a href=&#34;http://arapaima.uk/post/2016-10-25-database-refactoring-ssdt-dropping-objects/#a-note-on-drop-objects-not-in-source&#34;&gt;Drop objects in target but not in source&lt;/a&gt;&amp;rdquo;).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span class=&#34;n&#34;&gt;PRINT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Dropping [dbo].[PlaylistTrack]...&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;DROP&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PlaylistTrack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;PRINT&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Creating [dbo].[Playlist_Track]...&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;GO&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;CREATE&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;TABLE&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dbo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;].[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Playlist_Track&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PlaylistId&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;INT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NOT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrackId&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;    &lt;span class=&#34;nb&#34;&gt;INT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NOT&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;NULL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
    &lt;span class=&#34;k&#34;&gt;CONSTRAINT&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PK_PlaylistTrack&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;PRIMARY&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;KEY&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NONCLUSTERED&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;PlaylistId&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;ASC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TrackId&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;ASC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;renaming-programmable-objects-views-functions-stored-procedures-other-miscellany-in-sys-sql-modules&#34;&gt;Renaming Programmable Objects (Views, Functions, Stored Procedures, other miscellany in [&lt;code&gt;sys.sql_modules&lt;/code&gt;]&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Refactoring Databases, p 117&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As noted in the &lt;a href=&#34;http://arapaima.uk/post/2016-10-25-database-refactoring-ssdt-dropping-objects/#dropping-programmable-objects-views-functions-stored-procedures-other-miscellany-in-sys-sql-modules-https-msdn-microsoft-com-en-us-library-ms175081-aspx&#34;&gt;discussion of dropping programmable objects&lt;/a&gt;, operations involving these objects involve substantially less risk of catastrophic data loss and consequent unemployment than similar operation involving tables and columns.&lt;/p&gt;

&lt;p&gt;It is still important to use the Refactor &amp;rarr; Rename and Refactor &amp;rarr; Move to Schema techniques to rename these objects rather than directly editing the &lt;code&gt;.sql&lt;/code&gt; files, so that an entry is written to the &lt;code&gt;.refactorlog&lt;/code&gt; file. This will ensure that the existing object is altered rather than a new one created at the expense of the old one.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span class=&#34;nt&#34;&gt;&amp;lt;Operation&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Rename Refactor&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Key=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;fc4b928d-9b00-4028-9cac-5859ba9b666c&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;ChangeDateTime=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;11/12/2016 22:50:20&amp;#34;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo].[ChangeTrackPriceByFactor]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlProcedure&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ParentElementName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[dbo]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;ParentElementType&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;SqlSchema&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&#34;nt&#34;&gt;&amp;lt;Property&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Name=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;NewName&amp;#34;&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;Value=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;[ChangeTrackPriceByMultiplier]&amp;#34;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&#34;nt&#34;&gt;&amp;lt;/Operation&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;It isn&amp;rsquo;t magic, this doesn&amp;rsquo;t work with dynamic SQL, for instance.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Database Pruning</title>
          <link>http://arapaima.uk/post/2016-10-25-database-refactoring-ssdt-dropping-objects/</link>
          <pubDate>Tue, 25 Oct 2016 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2016-10-25-database-refactoring-ssdt-dropping-objects/</guid>
          <description>

&lt;p&gt;After a period of time in use, most databases, like many systems that have grown in an organic matter, can benefit from some judicious pruning. This can take the form of removing columns or even tables no longer required to support the application, or that store redundant - and hence possibly erroneous - copies of information stored elsewhere. Equally there may be stored procedures, functions, and even triggers that contain out-of-date versions of application logic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/7/70/The_gardener%27s_assistant%3B_a_practical_and_scientific_exposition_of_the_art_of_gardening_in_all_its_branches_%281910%29_%2814761716416%29.jpg&#34; alt=&#34;The gardener&#39;s assistant; a practical and scientific exposition of the art of gardening in all its branches (1910) (14761716416).jpg&#34; title=&#34;[By Internet Archive Book Images [No restrictions], via Wikimedia Commons]&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dropping-a-column&#34;&gt;Dropping a column&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Refactoring Databases, p 72&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since SSDT operates in a declarative manner, each table is defined in a &lt;code&gt;CREATE TABLE&lt;/code&gt; script, and deleting a column is as simple as deleting the relevant line from the script. However, there are a couple of features of SSDT that are relevant here. If the column is referenced by any other database objects such as views, functions, or stored procedures, SSDT will display an error:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/ssdt-refactoring-part-1/DropColumnReferencedByProcedure.PNG&#34; alt=&#34;SSDT Broken Reference Error&#34; title=&#34;SSDT Broken Reference Error&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The full text of the error message reads &lt;code&gt;SQL71501: Procedure [dbo.UpdateInvoiceBillingAddress] has an unresolved reference to object [dbo].[Invoice].[BillingPostalCode]&lt;/code&gt;, and the offending reference in the stored procedure&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; has acquired a &amp;ldquo;red squiggly&amp;rdquo;. There&amp;rsquo;s no &amp;ldquo;builtin&amp;rdquo; refactoring action defined in the right-click context menu for deletions, presumably as it&amp;rsquo;s difficult to programmatically determine the &amp;ldquo;intent&amp;rdquo; of a deleting a column.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/ssdt-refactoring-part-1/Refactoring+Menu.PNG&#34; alt=&#34;Refactoring Context Menu&#34; title=&#34;Refactoring Context Menu&#34; /&gt;&lt;/p&gt;

&lt;p&gt;What is possible, however, is to use the &amp;ldquo;find all references&amp;rdquo; tool &lt;em&gt;before&lt;/em&gt; deleting the column to enable us to take appropriate action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/ssdt-refactoring-part-1/findAllReferences.PNG&#34; alt=&#34;Find All References&#34; title=&#34;Find All References&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dropping-a-table&#34;&gt;Dropping a table&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Refactoring Databases, p 77&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is where things get more serious. The same technique of using the &amp;ldquo;Find All References&amp;rdquo; tool to assess the damage we&amp;rsquo;re about to do applies here, but there is a subtle difference in what happens when we actually brandish the pruning shears. It turns out that removing an entire table referenced by a stored procedure is only worthy of a warning or five rather than an error. This is due to an oddity of &lt;a href=&#34;https://technet.microsoft.com/en-us/library/ms190686.aspx&#34;&gt;deferred name resolution&lt;/a&gt; for stored procedures, namely that it is permitted to reference a non-existent table in the text of a stored procedure, but not permitted to reference a non-existent column in a table that &lt;em&gt;does&lt;/em&gt; exist.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/ssdt-refactoring-part-1/RemovingATableIsOnlyAWarning.PNG&#34; alt=&#34;Removing a table is only a warning&#34; title=&#34;Removing a table is only a warning&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, if we convert our stored procedure to a user-defined function &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; the warnings turn into errors. It turns out deferred name resolution doesn&amp;rsquo;t work at all for functions, presumably as being &amp;ldquo;purely functional&amp;rdquo; and not side-effecting it is less likely that there will be objects that exist at runtime but not at creation time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/ssdt-refactoring-part-1/FunctionsDontSupportDeferredNameResolution.PNG&#34; alt=&#34;Functions don&#39;t support deferred name resolution&#34; title=&#34;Functions don&#39;t support deferred name resolution&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dropping-programmable-objects-views-functions-stored-procedures-other-miscellany-in-sys-sql-modules-https-msdn-microsoft-com-en-us-library-ms175081-aspx&#34;&gt;Dropping Programmable Objects (Views, Functions, Stored Procedures, other miscellany in &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/ms175081.aspx&#34;&gt;&lt;code&gt;sys.sql_modules&lt;/code&gt;&lt;/a&gt;)&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Refactoring Databases, p 79&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;On the face of it this is simpler, as there is no data being thrown out with the bathwater.&lt;/p&gt;

&lt;h3 id=&#34;dropping-triggers&#34;&gt;Dropping Triggers&lt;/h3&gt;

&lt;p&gt;You should do this without hesitation. It&amp;rsquo;s 2016.&lt;/p&gt;

&lt;h3 id=&#34;dropping-views-and-functions&#34;&gt;Dropping views and functions&lt;/h3&gt;

&lt;p&gt;Generally, this is unproblematic. Since these don&amp;rsquo;t support deferred name resolution, you can get error checking in SSDT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://s3-eu-west-1.amazonaws.com/aksidjenakfjg/ssdt-refactoring-part-1/NameCheckingInViews.PNG&#34; alt=&#34;Name checking in views&#34; title=&#34;Name checking in views&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;pulling-the-trigger-3&#34;&gt;Pulling the trigger&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s one more thing to consider, which is what happens when we come to deploy our changes. Whether we do this by publishing from Visual Studio or at the command line using &lt;code&gt;sqlpackage.exe&lt;/code&gt;, if we are dropping a table or a column that contains data we will get an error along the lines of&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(48,1): SQL72014: .Net SqlClient Data Provider: Msg 50000, Level 16, State 127, Line 6 Rows were detected. The schema update is terminating because data loss might occur.
(43,0): SQL72045: Script execution error.  The executed script:
IF EXISTS (SELECT TOP 1 1
           FROM   [dbo].[PlaylistTrack])
    RAISERROR (N&#39;Rows were detected. The schema update is terminating because data loss might occur.&#39;, 16, 127)
        WITH NOWAIT;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;IF EXISTS&lt;/code&gt; check gets inserted into the deployment script for every table, and every table containing a column, that is being dropped. It is worth noting that since the check is for &lt;code&gt;EXISTS (SELECT TOP 1 1)&lt;/code&gt;, this check will fail and the deployment will be blocked even if we are dropping a column that only contains &lt;code&gt;NULL&lt;/code&gt; values - I have found this to be mildly irritating in the past, particularly for &amp;ldquo;inadvertently&amp;rdquo; created columns.&lt;/p&gt;

&lt;p&gt;To inhibit this behaviour and allow our potentially destructive change to proceed, we need to specify this at deploy time, the mechanism for which depends on the method we are using to deploy our project.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For projects deployed using the Visual Studio &amp;ldquo;Publish&amp;rdquo; dialog, uncheck &amp;ldquo;Block Incremental Deployment if Data Loss might Occur&amp;rdquo; in the &amp;ldquo;Advanced Publish Settings&amp;rdquo; dialog available by clicking &amp;ldquo;Advanced&amp;rdquo; in the &amp;ldquo;Publish&amp;rdquo; dialog.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For projects deployed using &lt;code&gt;sqlpackage.exe&lt;/code&gt;, we need to specify the parameter &lt;code&gt;/p:BlockOnPossibleDataLoss=False&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For projects that use a publish profile to specify deployment options, we need to add the element &lt;code&gt;&amp;lt;BlockOnPossibleDataLoss&amp;gt;False&amp;lt;/BlockOnPossibleDataLoss&amp;gt;&lt;/code&gt; to the &lt;code&gt;.publish.xml&lt;/code&gt; file.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In each case, the default is &amp;ldquo;true&amp;rdquo;, meaning potentially destructive changes are blocked by default. This is for the benefit of those &amp;ldquo;enterprise&amp;rdquo; customers that deploy direct to production with no testing - remember that these people aren&amp;rsquo;t our problem but they are the SSDT development team&amp;rsquo;s problem, since they are paying to keep the lights on at SSDT HQ!&lt;/p&gt;

&lt;p&gt;In general, this should always be set to false, meaning &amp;ldquo;allow potentially destructive changes&amp;rdquo;. This is unproblematic as long as production isn&amp;rsquo;t the &lt;em&gt;first&lt;/em&gt; environment where you deploy your changes.&lt;/p&gt;

&lt;p&gt;Similarly, the default behaviour of SSDT is &lt;em&gt;not&lt;/em&gt; to drop objects that are in the target (i.e. the database) but not in the source (i.e. the database project). To allow these changes to be applied, we need to do one of the following.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For projects deployed using the Visual Studio &amp;ldquo;Publish&amp;rdquo; dialog, check &amp;ldquo;Drop objects in target but not in source&amp;rdquo; in the &amp;ldquo;Advanced Publish Settings&amp;rdquo; dialog available by clicking &amp;ldquo;Advanced&amp;rdquo; in the &amp;ldquo;Publish&amp;rdquo; dialog.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For projects deployed using &lt;code&gt;sqlpackage.exe&lt;/code&gt;, we need to specify the parameter &lt;code&gt;/p:DropObjectsNotInSource=True&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;For projects that use a publish profile to specify deployment options, we need to add the element &lt;code&gt;&amp;lt;DropObjectsNotInSource&amp;gt;True&amp;lt;/DropObjectsNotInSource&amp;gt;&lt;/code&gt; to the &lt;code&gt;.publish.xml&lt;/code&gt; file.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;a-note-on-drop-objects-not-in-source&#34;&gt;A note on &amp;ldquo;Drop objects not in source&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;If this option is selected, SSDT will drop &lt;em&gt;all&lt;/em&gt; objects from the target database that are not defined in the project. Rather inconveniently, this includes users, permissions - including, cruciallly, the &lt;code&gt;CONNECT&lt;/code&gt; permission - and all the other things we need to be present for our application (or us!) to be able to connect to the database. Rather than specify all these items as part of the project, it is often simpler to ignore these at deployment time using the publish profile. The most important ones are probably users, permissions, roles, and role memberships, but anyone with a more obscure security model (Application Roles??) might want to investigate some of the other options.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://aksidjenakfjg.s3.amazonaws.com/ssdt-refactoring-part-1/Options-for-drop-objects.PNG&#34; alt=&#34;Advanced Publish Settings showing drop objects&#34; title=&#34;Advanced Publish Settings showing drop objects&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The difference between the &amp;ldquo;Drop&amp;rdquo; and &amp;ldquo;Ignore&amp;rdquo; settings is that the former apply &lt;em&gt;only&lt;/em&gt; to the target - so selecting object types here prevents objects of that type from being dropped. The &amp;ldquo;ignore&amp;rdquo; settings allow us to specify that objects of the selected types are not dropped in the target if they are absent from the source, but also not created or modified in the target even if they are present in the source.&lt;/p&gt;

&lt;p&gt;As above, these settings can also be specified as command-line arguments to &lt;code&gt;sqlpackage.exe&lt;/code&gt; or as xml in a publish profile.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;This procedure isn&amp;rsquo;t in the original Chinook database, I added it for the purpose of this example.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;which it should have been in the first place, as it doesn&amp;rsquo;t modify any data!
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;No, not that kind of trigger, this refers to the metaphorical kind.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Refactoring Databases with SSDT</title>
          <link>http://arapaima.uk/post/2016-10-21-database-refactoring-part-0/</link>
          <pubDate>Fri, 21 Oct 2016 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2016-10-21-database-refactoring-part-0/</guid>
          <description>&lt;p&gt;The book &lt;a href=&#34;http://www.pearsoned.co.uk/bookshop/detail.asp?WT.oss=refactoring%20databases&amp;amp;WT.oss_r=1&amp;amp;item=100000000444392&#34;&gt;Refactoring Databases&lt;/a&gt; by Scott Ambler and Pramod Sadalage, first published over ten years ago, has become something of a modern classic in the field of agile database delivery. The authors give a definition (in fact taken from an &lt;a href=&#34;http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471202835.html&#34;&gt;earlier book&lt;/a&gt;) of a database refactoring as&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;hellip;a simple change to a database schema that improves its design while retaining both its behavioral and informational semantics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The book is divided into two sections, the first being a discussion of agile database development techniques, placing database refactoring in a wider technical and organisational contect. This material, intended to be read in order, is recommended reading for anyone struggling to improve the working practices associated with database delivery in any organisation, irrespective of the tools being used.&lt;/p&gt;

&lt;p&gt;The second is a collection of named &amp;ldquo;refactorings&amp;rdquo; along with the steps required to implement each one, and is structured as a reference work rather than as a continuous narrative. There are online versions of this catalog maintained at the websites of &lt;a href=&#34;http://www.agiledata.org/essays/databaseRefactoringCatalog.html&#34;&gt;Scott Ambler&lt;/a&gt; and &lt;a href=&#34;http://databaserefactoring.com/&#34;&gt;Pramod Sadalage&lt;/a&gt; respectively.&lt;/p&gt;

&lt;p&gt;This article marks the start of a series that examines a number of these named &amp;ldquo;refactorings&amp;rdquo; and looks at how they can be implemented using Microsoft® SQL Server and Microsoft® Visual Studio, in particular through the use of &lt;a href=&#34;https://blogs.msdn.microsoft.com/ssdt/&#34;&gt;SQL Server Data Tools&lt;/a&gt;, known as &amp;ldquo;SSDT&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The code examples in the book use Oracle with bits of Java and Hibernate thrown in as appropriate, so the procedures here will differ where the behaviour of SQL Server differs in some relevant way. In addition, the &amp;ldquo;&lt;a href=&#34;https://blogs.msdn.microsoft.com/gertd/2009/06/05/declarative-database-development&#34; title=&#34;The original DataDude article from way back&#34;&gt;declarative&lt;/a&gt;&amp;rdquo; development paradigm encouraged by SSDT hides a lot of the detail of the DDL behind the scenes, meaning that the steps outlined in the book to achieve each refactoring may not all occur in the same places.&lt;/p&gt;

&lt;p&gt;That said, it is worth referring to the discussions of each individual refactoring contained in the book, since these provide a checklist of considerations before embarking on any change, including motivations, tradeoffs, schema update and data migration mechanics, and required application changes, most of which are outside the scope of SSDT. I will attempt to provide page numbers where appropriate, though I realise these are less useful for those with access to only an electronic copy of the book.&lt;/p&gt;

&lt;p&gt;In general, the applicability of SSDT is restricted to managing schema changes, with a couple of extensibility points - namely &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/jj889461(v=vs.103).aspx&#34;&gt;Pre-deployment and Post-deployment scripts&lt;/a&gt; -  to manage data movements.&lt;/p&gt;

&lt;p&gt;This series does presuppose some experience with SSDT; an overview can be obtained from MSDN &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/hh272686(v=vs.103).aspx&#34;&gt;here&lt;/a&gt;, particularly the material relating to &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/hh272702(v=vs.103).aspx&#34;&gt;Project-Oriented Offline Database Development&lt;/a&gt;. In the event that I ever get around to writing any introductory material of my own I&amp;rsquo;ll come back and add links here as appropriate. Most of the examples use the &amp;ldquo;Chinook&amp;rdquo; sample database available from &lt;a href=&#34;https://chinookdatabase.codeplex.com/&#34;&gt;codeplex&lt;/a&gt;. The SSDT project I am using is on &lt;a href=&#34;https://github.com/arapaima-uk/Refactoring.Chinook&#34; title=&#34;Link to the Refactoring.Chinook repo&#34;&gt;Github&lt;/a&gt;, but note that I&amp;rsquo;ve made no attempt to make the commits sync with the articles in the series, and there are no guarantees, express or implied, that any given commit will actually be buildable.&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Verschlimmbesserung</title>
          <link>http://arapaima.uk/post/2016-10-13-verschlimmbesserung/</link>
          <pubDate>Thu, 13 Oct 2016 00:00:00 UTC</pubDate>
          <author>Gavin Campbell</author>
          <guid>http://arapaima.uk/post/2016-10-13-verschlimmbesserung/</guid>
          <description>&lt;p&gt;Welcome to this, my new site. I&amp;rsquo;ll be adding content about Continuous Integration, database systems including but not limited to Microsoft® SQL Server and development tools including but not limited to Microsoft® Visual Studio. The site itself is provisioned in a moderately interesting way, which you can read about &lt;a href=&#34;http://arapaima.uk/fixed/blogsetup/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
      
    
      
    

  </channel>
</rss>
